{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Http using Python vs Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\n",
      "  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\n",
      "\n",
      "\n",
      "\n",
      "  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-BSy+E+S5PJuDWKcXiIXBoFJ7uJ+88y6hFdIhZpf7nf9MVNVvnJDPUaotaxFUQi8UXCLJOcGv1uifxVMc9o5DYQ==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-052cbe13e4b93c9b8358a7178885c1a0.css\" />\n",
      "  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-aQX2djfPBL4OvvrSedVieBg1MykiSlcGcyx8r0NNVSbPQr2bjw4KhbEdz+MoACuZ3IXmu3z3qzou9BK618oJWw==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/site-6905f67637cf04be0ebefad279d56278.css\" />\n",
      "    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-bRic7lVTQ3HMC+Xi6jXFdLpMFO5Yl6b+apchURCSN1cjAKteDgORh3nfXWuG1zLqbEVHYXWC9G/W1VQ3IYd32Q==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/github-6d189cee55534371cc0be5e2ea35c574.css\" />\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "  <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-8K2vvwbW+6H27Nad5ydg8PA2/aMD/LKq+EiK9s0U0hhVZxCI2tWBsYk9beAtisRw2j+Or5k2/F+6dk02nmj/PA==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/environment-f0adafbf.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-bf3k8Y60Hljk6JoJikJwUktgpWsNM3NEnNgmlScAD6u57WAUEgCSZh5BsU5ls8YxsL8cZUaKGHC5fsf143pfaw==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/chunk-frameworks-6dfde4f1.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-TZfq2T+XE31Mcp17bGlABjjeQ/69IH2f/sBvS31c4Jua0e1O/NE+aVbQtItMM2GFlmeEm13p/zyyWGIx10jVKQ==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/chunk-vendor-4d97ead9.js\"></script>\n",
      "  \n",
      "  <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-wZC6mpo8eFbRXFkq9Wkvlwz/cXRmtqM5K00xtcy1QVl3UUnA1Z4jYDwPQhqk4zSuZ5vv7abBD1mQjD4VWug58A==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/behaviors-c190ba9a.js\"></script>\n",
      "  \n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-F/8sNCLF603gJL0egQeZwFTYP9dfZAVEWHdjMssZ2opLZXRK08vgowBRzKyra6u9zU/rGVxe+igfJsSWzfO2lg==\" type=\"application/javascript\" data-module-id=\"./chunk-contributions-spider-graph.js\" data-src=\"https://github.githubassets.com/assets/chunk-contributions-spider-graph-17ff2c34.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-obMR8mPKx8OvqRe34LgnUcxeJ1qujiA4ND3H6UX13ExMlA/WfHLjEzXRmgGRcRvN/8J1nzc+Z+jgz/PLTFy6zg==\" type=\"application/javascript\" data-module-id=\"./chunk-drag-drop.js\" data-src=\"https://github.githubassets.com/assets/chunk-drag-drop-a1b311f2.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-AVtc/43rg+h7XgMPOa+baO1Fj+uiMUrLHd16U6+c6Tk7atk70Ux4IcGRPOHqMv422QIJD7l7BhG2PL4G/tUVKg==\" type=\"application/javascript\" data-module-id=\"./chunk-jump-to.js\" data-src=\"https://github.githubassets.com/assets/chunk-jump-to-015b5cff.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-biWH4OAQPiDiTkaeyDnXf6hHF3u3Mv3eaDEvt8AdT/QvWDpLbnusALT4ZO+f+DgYp07UiHxNRAB6/J7IwPW6TQ==\" type=\"application/javascript\" data-module-id=\"./chunk-lib.dev.js\" data-src=\"https://github.githubassets.com/assets/chunk-lib.dev-6e2587e0.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-tcH4xCRuMBAh1PruDaiwGnRIbHlF6bGLhxyCQ16uqok1cV5QFMguVPWJtN9KI0jGQOgN+Pha3+uOUXhXdfK/qw==\" type=\"application/javascript\" data-module-id=\"./chunk-profile-pins-element.js\" data-src=\"https://github.githubassets.com/assets/chunk-profile-pins-element-b5c1f8c4.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-E+H+wAtjiqutBvn2cnXzDIvmasIhYiS7i7JzOfFUwo+Ej8zT54OrJtP//RhwixnypgOpCF4JvqzYy6zOtORDmg==\" type=\"application/javascript\" data-module-id=\"./chunk-runner-groups.js\" data-src=\"https://github.githubassets.com/assets/chunk-runner-groups-13e1fec0.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-rIWotuwsYxV+jXNuevOVsulW2zsw98Gi4j7KXrJZq+bWFV86Y/R75+1jMjt5k1MPgabdTPpWPI/IoESZhah9ng==\" type=\"application/javascript\" data-module-id=\"./chunk-sortable-behavior.js\" data-src=\"https://github.githubassets.com/assets/chunk-sortable-behavior-ac85a8b6.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-ZBICwPA18KYIgHbafqdjnTkxoCSNf/39rTlupkrtdpIWizECoux0ROzjAgyAx20Bf27VfshuBqrYwDqOVftyag==\" type=\"application/javascript\" data-module-id=\"./chunk-tweetsodium.js\" data-src=\"https://github.githubassets.com/assets/chunk-tweetsodium-641202c0.js\"></script>\n",
      "    <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-QUtHMb3mxbE2DQSQZb9+tK8ZC2m05FFqZOa5ju2Sj0GdiQV+ce/bnIK/YwoMdty44NEhEIkZUIyh9DqLMsRedg==\" type=\"application/javascript\" data-module-id=\"./chunk-user-status-submit.js\" data-src=\"https://github.githubassets.com/assets/chunk-user-status-submit-414b4731.js\"></script>\n",
      "  \n",
      "  <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-B+TgAy+wlZmUjsDcWKt55BrTpxxZDolcD9v47x25Ue46lVb9c0n1tRz+BjjSeC4CuN9M3psDUpBhsr1aFKpFCw==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/repositories-07e4e003.js\"></script>\n",
      "<script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-Dl4JXoI2igFuQLn6MCPPgiS7TVx3c0/G8iaIZCNzgGi4Iwg8TC0y4LPT584yJ+r6ZsF5QIFL9uo+nhHdBvfLNQ==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/diffs-0e5e095e.js\"></script>\n",
      "\n",
      "  <meta name=\"viewport\" content=\"width=device-width\">\n",
      "  \n",
      "  <title>pysplice/README.md at master · splicemachine/pysplice · GitHub</title>\n",
      "    <meta name=\"description\" content=\"Contribute to splicemachine/pysplice development by creating an account on GitHub.\">\n",
      "    <link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/opensearch.xml\" title=\"GitHub\">\n",
      "  <link rel=\"fluid-icon\" href=\"https://github.com/fluidicon.png\" title=\"GitHub\">\n",
      "  <meta property=\"fb:app_id\" content=\"1401488693436528\">\n",
      "  <meta name=\"apple-itunes-app\" content=\"app-id=1477376905\" />\n",
      "    <meta name=\"twitter:image:src\" content=\"https://avatars2.githubusercontent.com/u/3579368?s=400&amp;v=4\" /><meta name=\"twitter:site\" content=\"@github\" /><meta name=\"twitter:card\" content=\"summary\" /><meta name=\"twitter:title\" content=\"splicemachine/pysplice\" /><meta name=\"twitter:description\" content=\"Contribute to splicemachine/pysplice development by creating an account on GitHub.\" />\n",
      "    <meta property=\"og:image\" content=\"https://avatars2.githubusercontent.com/u/3579368?s=400&amp;v=4\" /><meta property=\"og:site_name\" content=\"GitHub\" /><meta property=\"og:type\" content=\"object\" /><meta property=\"og:title\" content=\"splicemachine/pysplice\" /><meta property=\"og:url\" content=\"https://github.com/splicemachine/pysplice\" /><meta property=\"og:description\" content=\"Contribute to splicemachine/pysplice development by creating an account on GitHub.\" />\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "  <link rel=\"assets\" href=\"https://github.githubassets.com/\">\n",
      "  \n",
      "\n",
      "  <meta name=\"request-id\" content=\"CE80:4FAC:181ED3C:22CCC0D:5FDE14A0\" data-pjax-transient=\"true\"/><meta name=\"html-safe-nonce\" content=\"2e26e2f0ee132e83d8ca968f489040c38a2f53eea3977a7712b4b20fd9805317\" data-pjax-transient=\"true\"/><meta name=\"visitor-payload\" content=\"eyJyZWZlcnJlciI6IiIsInJlcXVlc3RfaWQiOiJDRTgwOjRGQUM6MTgxRUQzQzoyMkNDQzBEOjVGREUxNEEwIiwidmlzaXRvcl9pZCI6IjMzMjYwODYxNzY3MjIyNjExNTIiLCJyZWdpb25fZWRnZSI6ImlhZCIsInJlZ2lvbl9yZW5kZXIiOiJpYWQifQ==\" data-pjax-transient=\"true\"/><meta name=\"visitor-hmac\" content=\"080c3e3fc4ba82dd608969e461d2b90612369dad232cdec89290b95b7a8dec48\" data-pjax-transient=\"true\"/>\n",
      "\n",
      "    <meta name=\"hovercard-subject-tag\" content=\"repository:142071553\" data-pjax-transient>\n",
      "\n",
      "\n",
      "  <meta name=\"github-keyboard-shortcuts\" content=\"repository,source-code\" data-pjax-transient=\"true\" />\n",
      "\n",
      "  \n",
      "\n",
      "  <meta name=\"selected-link\" value=\"repo_source\" data-pjax-transient>\n",
      "\n",
      "    <meta name=\"google-site-verification\" content=\"c1kuD-K2HIVF635lypcsWPoD4kilo5-jA_wBFyT4uMY\">\n",
      "  <meta name=\"google-site-verification\" content=\"KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU\">\n",
      "  <meta name=\"google-site-verification\" content=\"ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA\">\n",
      "  <meta name=\"google-site-verification\" content=\"GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc\">\n",
      "\n",
      "  <meta name=\"octolytics-host\" content=\"collector.githubapp.com\" /><meta name=\"octolytics-app-id\" content=\"github\" /><meta name=\"octolytics-event-url\" content=\"https://collector.githubapp.com/github-external/browser_event\" />\n",
      "\n",
      "  <meta name=\"analytics-location\" content=\"/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show\" data-pjax-transient=\"true\" />\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "  <meta name=\"optimizely-datafile\" content=\"{&quot;version&quot;: &quot;4&quot;, &quot;rollouts&quot;: [], &quot;typedAudiences&quot;: [], &quot;anonymizeIP&quot;: true, &quot;projectId&quot;: &quot;16737760170&quot;, &quot;variables&quot;: [], &quot;featureFlags&quot;: [], &quot;experiments&quot;: [{&quot;status&quot;: &quot;Running&quot;, &quot;audienceIds&quot;: [], &quot;variations&quot;: [{&quot;variables&quot;: [], &quot;id&quot;: &quot;19512260014&quot;, &quot;key&quot;: &quot;separate_plans_and_usage&quot;}, {&quot;variables&quot;: [], &quot;id&quot;: &quot;19509770036&quot;, &quot;key&quot;: &quot;control&quot;}], &quot;id&quot;: &quot;19536000020&quot;, &quot;key&quot;: &quot;account_billing_plans_and_usage&quot;, &quot;layerId&quot;: &quot;19545220022&quot;, &quot;trafficAllocation&quot;: [{&quot;entityId&quot;: &quot;19509770036&quot;, &quot;endOfRange&quot;: 2500}, {&quot;entityId&quot;: &quot;&quot;, &quot;endOfRange&quot;: 5000}, {&quot;entityId&quot;: &quot;19512260014&quot;, &quot;endOfRange&quot;: 7500}, {&quot;entityId&quot;: &quot;&quot;, &quot;endOfRange&quot;: 10000}], &quot;forcedVariations&quot;: {&quot;1238720267648ea2c88a74b410aa3c5c&quot;: &quot;separate_plans_and_usage&quot;}}], &quot;audiences&quot;: [{&quot;conditions&quot;: &quot;[\\&quot;or\\&quot;, {\\&quot;match\\&quot;: \\&quot;exact\\&quot;, \\&quot;name\\&quot;: \\&quot;$opt_dummy_attribute\\&quot;, \\&quot;type\\&quot;: \\&quot;custom_attribute\\&quot;, \\&quot;value\\&quot;: \\&quot;$opt_dummy_value\\&quot;}]&quot;, &quot;id&quot;: &quot;$opt_dummy_audience&quot;, &quot;name&quot;: &quot;Optimizely-Generated Audience for Backwards Compatibility&quot;}], &quot;groups&quot;: [], &quot;attributes&quot;: [{&quot;id&quot;: &quot;16822470375&quot;, &quot;key&quot;: &quot;user_id&quot;}, {&quot;id&quot;: &quot;17143601254&quot;, &quot;key&quot;: &quot;spammy&quot;}, {&quot;id&quot;: &quot;18175660309&quot;, &quot;key&quot;: &quot;organization_plan&quot;}, {&quot;id&quot;: &quot;18813001570&quot;, &quot;key&quot;: &quot;is_logged_in&quot;}, {&quot;id&quot;: &quot;19073851829&quot;, &quot;key&quot;: &quot;geo&quot;}], &quot;botFiltering&quot;: false, &quot;accountId&quot;: &quot;16737760170&quot;, &quot;events&quot;: [{&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;17911811441&quot;, &quot;key&quot;: &quot;hydro_click.dashboard.teacher_toolbox_cta&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18124116703&quot;, &quot;key&quot;: &quot;submit.organizations.complete_sign_up&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18145892387&quot;, &quot;key&quot;: &quot;no_metric.tracked_outside_of_optimizely&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18178755568&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.add_repo&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18180553241&quot;, &quot;key&quot;: &quot;submit.repository_imports.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18186103728&quot;, &quot;key&quot;: &quot;click.help.learn_more_about_repository_creation&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18188530140&quot;, &quot;key&quot;: &quot;test_event.do_not_use_in_production&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18191963644&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.transfer_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18195612788&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.import_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18210945499&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.invite_members&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18211063248&quot;, &quot;key&quot;: &quot;click.empty_org_repo_cta.create_repository&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18215721889&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.update_profile&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18224360785&quot;, &quot;key&quot;: &quot;click.org_onboarding_checklist.dismiss&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18234832286&quot;, &quot;key&quot;: &quot;submit.organization_activation.complete&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18252392383&quot;, &quot;key&quot;: &quot;submit.org_repository.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18257551537&quot;, &quot;key&quot;: &quot;submit.org_member_invitation.create&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18259522260&quot;, &quot;key&quot;: &quot;submit.organization_profile.update&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18564603625&quot;, &quot;key&quot;: &quot;view.classroom_select_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18568612016&quot;, &quot;key&quot;: &quot;click.classroom_sign_in_click&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18572592540&quot;, &quot;key&quot;: &quot;view.classroom_name&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18574203855&quot;, &quot;key&quot;: &quot;click.classroom_create_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18582053415&quot;, &quot;key&quot;: &quot;click.classroom_select_organization&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18589463420&quot;, &quot;key&quot;: &quot;click.classroom_create_classroom&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18591323364&quot;, &quot;key&quot;: &quot;click.classroom_create_first_classroom&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18591652321&quot;, &quot;key&quot;: &quot;click.classroom_grant_access&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18607131425&quot;, &quot;key&quot;: &quot;view.classroom_creation&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;18831680583&quot;, &quot;key&quot;: &quot;upgrade_account_plan&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19064064515&quot;, &quot;key&quot;: &quot;click.signup&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19075373687&quot;, &quot;key&quot;: &quot;click.view_account_billing_page&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19077355841&quot;, &quot;key&quot;: &quot;click.dismiss_signup_prompt&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19079713938&quot;, &quot;key&quot;: &quot;click.contact_sales&quot;}, {&quot;experimentIds&quot;: [&quot;19536000020&quot;], &quot;id&quot;: &quot;19120963070&quot;, &quot;key&quot;: &quot;click.compare_account_plans&quot;}, {&quot;experimentIds&quot;: [&quot;19536000020&quot;], &quot;id&quot;: &quot;19151690317&quot;, &quot;key&quot;: &quot;click.upgrade_account_cta&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19424193129&quot;, &quot;key&quot;: &quot;click.open_account_switcher&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19520330825&quot;, &quot;key&quot;: &quot;click.visit_account_profile&quot;}, {&quot;experimentIds&quot;: [], &quot;id&quot;: &quot;19540970635&quot;, &quot;key&quot;: &quot;click.switch_account_context&quot;}], &quot;revision&quot;: &quot;380&quot;}\" />\n",
      "  <!-- To prevent page flashing, the optimizely JS needs to be loaded in the\n",
      "    <head> tag before the DOM renders -->\n",
      "  <script crossorigin=\"anonymous\" defer=\"defer\" integrity=\"sha512-VJrqSK702Mzl9EQxm2OvFxKaumGptgVdeJS2rsaLvVlOdR4HEu3ZFjtV83kMKdYRelUnxxaAFw0wthkpdEUafw==\" type=\"application/javascript\" src=\"https://github.githubassets.com/assets/optimizely-549aea48.js\"></script>\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "      <meta name=\"hostname\" content=\"github.com\">\n",
      "    <meta name=\"user-login\" content=\"\">\n",
      "\n",
      "\n",
      "      <meta name=\"expected-hostname\" content=\"github.com\">\n",
      "\n",
      "\n",
      "    <meta name=\"enabled-features\" content=\"MARKETPLACE_PENDING_INSTALLATIONS,ACTIONS_ENVIRONMENTS,ACTIONS_ENVIRONMENTS\">\n",
      "\n",
      "  <meta http-equiv=\"x-pjax-version\" content=\"72da585936cb4f0f8e579bc15c2e733ed8e2169930cee94d77bccfac85971673\">\n",
      "  \n",
      "\n",
      "        <link href=\"https://github.com/splicemachine/pysplice/commits/master.atom\" rel=\"alternate\" title=\"Recent Commits to pysplice:master\" type=\"application/atom+xml\">\n",
      "\n",
      "  <meta name=\"go-import\" content=\"github.com/splicemachine/pysplice git https://github.com/splicemachine/pysplice.git\">\n",
      "\n",
      "  <meta name=\"octolytics-dimension-user_id\" content=\"3579368\" /><meta name=\"octolytics-dimension-user_login\" content=\"splicemachine\" /><meta name=\"octolytics-dimension-repository_id\" content=\"142071553\" /><meta name=\"octolytics-dimension-repository_nwo\" content=\"splicemachine/pysplice\" /><meta name=\"octolytics-dimension-repository_public\" content=\"true\" /><meta name=\"octolytics-dimension-repository_is_fork\" content=\"false\" /><meta name=\"octolytics-dimension-repository_network_root_id\" content=\"142071553\" /><meta name=\"octolytics-dimension-repository_network_root_nwo\" content=\"splicemachine/pysplice\" /><meta name=\"octolytics-dimension-repository_explore_github_marketplace_ci_cta_shown\" content=\"false\" />\n",
      "\n",
      "\n",
      "\n",
      "    <link rel=\"canonical\" href=\"https://github.com/splicemachine/pysplice/blob/master/README.md\" data-pjax-transient>\n",
      "\n",
      "\n",
      "  <meta name=\"browser-stats-url\" content=\"https://api.github.com/_private/browser/stats\">\n",
      "\n",
      "  <meta name=\"browser-errors-url\" content=\"https://api.github.com/_private/browser/errors\">\n",
      "\n",
      "  <meta name=\"browser-optimizely-client-errors-url\" content=\"https://api.github.com/_private/browser/optimizely_client/errors\">\n",
      "\n",
      "  <link rel=\"mask-icon\" href=\"https://github.githubassets.com/pinned-octocat.svg\" color=\"#000000\">\n",
      "  <link rel=\"alternate icon\" class=\"js-site-favicon\" type=\"image/png\" href=\"https://github.githubassets.com/favicons/favicon.png\">\n",
      "  <link rel=\"icon\" class=\"js-site-favicon\" type=\"image/svg+xml\" href=\"https://github.githubassets.com/favicons/favicon.svg\">\n",
      "\n",
      "<meta name=\"theme-color\" content=\"#1e2327\">\n",
      "  <meta name=\"color-scheme\" content=\"light dark\">\n",
      "\n",
      "\n",
      "  <link rel=\"manifest\" href=\"/manifest.json\" crossOrigin=\"use-credentials\">\n",
      "\n",
      "  </head>\n",
      "\n",
      "  <body class=\"logged-out env-production page-responsive page-blob\">\n",
      "    \n",
      "\n",
      "    <div class=\"position-relative js-header-wrapper \">\n",
      "      <a href=\"#start-of-content\" class=\"px-2 py-4 bg-blue text-white show-on-focus js-skip-to-content\">Skip to content</a>\n",
      "      <span class=\"progress-pjax-loader width-full js-pjax-loader-bar Progress position-fixed\">\n",
      "    <span style=\"background-color: #79b8ff;width: 0%;\" class=\"Progress-item progress-pjax-loader-bar \"></span>\n",
      "</span>      \n",
      "      \n",
      "\n",
      "\n",
      "            <header class=\"Header-old header-logged-out js-details-container Details position-relative f4 py-2\" role=\"banner\">\n",
      "  <div class=\"container-xl d-lg-flex flex-items-center p-responsive\">\n",
      "    <div class=\"d-flex flex-justify-between flex-items-center\">\n",
      "        <a class=\"mr-4\" href=\"https://github.com/\" aria-label=\"Homepage\" data-ga-click=\"(Logged out) Header, go to homepage, icon:logo-wordmark\">\n",
      "          <svg height=\"32\" class=\"octicon octicon-mark-github text-white\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"32\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\"></path></svg>\n",
      "        </a>\n",
      "\n",
      "          <div class=\"d-lg-none css-truncate css-truncate-target width-fit p-2\">\n",
      "            \n",
      "\n",
      "          </div>\n",
      "\n",
      "        <div class=\"d-flex flex-items-center\">\n",
      "              <a href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo\"\n",
      "                class=\"d-inline-block d-lg-none f5 text-white no-underline border border-gray-dark rounded-2 px-2 py-1 mr-3 mr-sm-5\"\n",
      "                data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"feb2be3729f79d784b3d54a7feb7f4060887073c8b4ccdf0de28d3e746377924\"\n",
      "                data-ga-click=\"Sign up, click to sign up for account, ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show;ref_cta:Sign up;ref_loc:header logged out\">\n",
      "                Sign&nbsp;up\n",
      "              </a>\n",
      "\n",
      "          <button class=\"btn-link d-lg-none mt-1 js-details-target\" type=\"button\" aria-label=\"Toggle navigation\" aria-expanded=\"false\">\n",
      "            <svg height=\"24\" class=\"octicon octicon-three-bars text-white\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z\"></path></svg>\n",
      "          </button>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"HeaderMenu HeaderMenu--logged-out position-fixed top-0 right-0 bottom-0 height-fit position-lg-relative d-lg-flex flex-justify-between flex-items-center flex-auto\">\n",
      "      <div class=\"d-flex d-lg-none flex-justify-end border-bottom bg-gray-light p-3\">\n",
      "        <button class=\"btn-link js-details-target\" type=\"button\" aria-label=\"Toggle navigation\" aria-expanded=\"false\">\n",
      "          <svg height=\"24\" class=\"octicon octicon-x text-gray\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"24\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M5.72 5.72a.75.75 0 011.06 0L12 10.94l5.22-5.22a.75.75 0 111.06 1.06L13.06 12l5.22 5.22a.75.75 0 11-1.06 1.06L12 13.06l-5.22 5.22a.75.75 0 01-1.06-1.06L10.94 12 5.72 6.78a.75.75 0 010-1.06z\"></path></svg>\n",
      "        </button>\n",
      "      </div>\n",
      "\n",
      "        <nav class=\"mt-0 px-3 px-lg-0 mb-5 mb-lg-0\" aria-label=\"Global\">\n",
      "          <ul class=\"d-lg-flex list-style-none\">\n",
      "              <li class=\"d-block d-lg-flex flex-lg-nowrap flex-lg-items-center border-bottom border-lg-bottom-0 mr-0 mr-lg-3 edge-item-fix position-relative flex-wrap flex-justify-between d-flex flex-items-center \">\n",
      "                <details class=\"HeaderMenu-details details-overlay details-reset width-full\">\n",
      "                  <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
      "                    Why GitHub?\n",
      "                    <svg x=\"0px\" y=\"0px\" viewBox=\"0 0 14 8\" xml:space=\"preserve\" fill=\"none\" class=\"icon-chevon-down-mktg position-absolute position-lg-relative\">\n",
      "                      <path d=\"M1,1l6.2,6L13,1\"></path>\n",
      "                    </svg>\n",
      "                  </summary>\n",
      "                  <div class=\"dropdown-menu flex-auto rounded-1 bg-white px-0 mt-0 pb-4 p-lg-4 position-relative position-lg-absolute left-0 left-lg-n4\">\n",
      "                    <a href=\"/features\" class=\"py-2 lh-condensed-ultra d-block link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Features\">Features <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a>\n",
      "                    <ul class=\"list-style-none f5 pb-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/code-review/\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Code review\">Code review</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/project-management/\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Project management\">Project management</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/integrations\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Integrations\">Integrations</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/actions\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Actions\">Actions</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/packages\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to GitHub Packages\">Packages</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features/security\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Security\">Security</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features#team-management\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Team management\">Team management</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/features#hosting\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Code hosting\">Hosting</a></li>\n",
      "                      <li class=\"edge-item-fix hide-xl\"><a href=\"/mobile\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Mobile\">Mobile</a></li>\n",
      "                    </ul>\n",
      "\n",
      "                    <ul class=\"list-style-none mb-0 border-lg-top pt-lg-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/customer-stories\" class=\"py-2 lh-condensed-ultra d-block no-underline link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Customer stories\">Customer stories <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/security\" class=\"py-2 lh-condensed-ultra d-block no-underline link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Security\">Security <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a></li>\n",
      "                    </ul>\n",
      "                  </div>\n",
      "                </details>\n",
      "              </li>\n",
      "              <li class=\"border-bottom border-lg-bottom-0 mr-0 mr-lg-3\">\n",
      "                <a href=\"/team\" class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Team\">Team</a>\n",
      "              </li>\n",
      "              <li class=\"border-bottom border-lg-bottom-0 mr-0 mr-lg-3\">\n",
      "                <a href=\"/enterprise\" class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Enterprise\">Enterprise</a>\n",
      "              </li>\n",
      "\n",
      "              <li class=\"d-block d-lg-flex flex-lg-nowrap flex-lg-items-center border-bottom border-lg-bottom-0 mr-0 mr-lg-3 edge-item-fix position-relative flex-wrap flex-justify-between d-flex flex-items-center \">\n",
      "                <details class=\"HeaderMenu-details details-overlay details-reset width-full\">\n",
      "                  <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
      "                    Explore\n",
      "                    <svg x=\"0px\" y=\"0px\" viewBox=\"0 0 14 8\" xml:space=\"preserve\" fill=\"none\" class=\"icon-chevon-down-mktg position-absolute position-lg-relative\">\n",
      "                      <path d=\"M1,1l6.2,6L13,1\"></path>\n",
      "                    </svg>\n",
      "                  </summary>\n",
      "\n",
      "                  <div class=\"dropdown-menu flex-auto rounded-1 bg-white px-0 pt-2 pb-0 mt-0 pb-4 p-lg-4 position-relative position-lg-absolute left-0 left-lg-n4\">\n",
      "                    <ul class=\"list-style-none mb-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/explore\" class=\"py-2 lh-condensed-ultra d-block link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Explore\">Explore GitHub <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a></li>\n",
      "                    </ul>\n",
      "\n",
      "                    <h4 class=\"text-gray-light text-normal text-mono f5 mb-2 border-lg-top pt-lg-3\">Learn &amp; contribute</h4>\n",
      "                    <ul class=\"list-style-none mb-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/topics\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Topics\">Topics</a></li>\n",
      "                        <li class=\"edge-item-fix\"><a href=\"/collections\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Collections\">Collections</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/trending\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Trending\">Trending</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://lab.github.com/\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Learning lab\">Learning Lab</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://opensource.guide\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Open source guides\">Open source guides</a></li>\n",
      "                    </ul>\n",
      "\n",
      "                    <h4 class=\"text-gray-light text-normal text-mono f5 mb-2 border-lg-top pt-lg-3\">Connect with others</h4>\n",
      "                    <ul class=\"list-style-none mb-0\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://github.com/events\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Events\">Events</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://github.community\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Community forum\">Community forum</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://education.github.com\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to GitHub Education\">GitHub Education</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://stars.github.com\" class=\"py-2 pb-0 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to GitHub Stars Program\">GitHub Stars program</a></li>\n",
      "                    </ul>\n",
      "                  </div>\n",
      "                </details>\n",
      "              </li>\n",
      "\n",
      "              <li class=\"border-bottom border-lg-bottom-0 mr-0 mr-lg-3\">\n",
      "                <a href=\"/marketplace\" class=\"HeaderMenu-link no-underline py-3 d-block d-lg-inline-block\" data-ga-click=\"(Logged out) Header, go to Marketplace\">Marketplace</a>\n",
      "              </li>\n",
      "\n",
      "              <li class=\"d-block d-lg-flex flex-lg-nowrap flex-lg-items-center border-bottom border-lg-bottom-0 mr-0 mr-lg-3 edge-item-fix position-relative flex-wrap flex-justify-between d-flex flex-items-center \">\n",
      "                <details class=\"HeaderMenu-details details-overlay details-reset width-full\">\n",
      "                  <summary class=\"HeaderMenu-summary HeaderMenu-link px-0 py-3 border-0 no-wrap d-block d-lg-inline-block\">\n",
      "                    Pricing\n",
      "                    <svg x=\"0px\" y=\"0px\" viewBox=\"0 0 14 8\" xml:space=\"preserve\" fill=\"none\" class=\"icon-chevon-down-mktg position-absolute position-lg-relative\">\n",
      "                       <path d=\"M1,1l6.2,6L13,1\"></path>\n",
      "                    </svg>\n",
      "                  </summary>\n",
      "\n",
      "                  <div class=\"dropdown-menu flex-auto rounded-1 bg-white px-0 pt-2 pb-4 mt-0 p-lg-4 position-relative position-lg-absolute left-0 left-lg-n4\">\n",
      "                    <a href=\"/pricing\" class=\"pb-2 lh-condensed-ultra d-block link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Pricing\">Plans <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a>\n",
      "\n",
      "                    <ul class=\"list-style-none mb-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/pricing#feature-comparison\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Compare plans\">Compare plans</a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://enterprise.github.com/contact\" class=\"py-2 lh-condensed-ultra d-block link-gray no-underline f5\" data-ga-click=\"(Logged out) Header, go to Contact Sales\">Contact Sales</a></li>\n",
      "                    </ul>\n",
      "\n",
      "                    <ul class=\"list-style-none mb-0 border-lg-top pt-lg-3\">\n",
      "                      <li class=\"edge-item-fix\"><a href=\"/nonprofit\" class=\"py-2 lh-condensed-ultra d-block no-underline link-gray-dark no-underline h5 Bump-link--hover\" data-ga-click=\"(Logged out) Header, go to Nonprofits\">Nonprofit <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a></li>\n",
      "                      <li class=\"edge-item-fix\"><a href=\"https://education.github.com\" class=\"py-2 pb-0 lh-condensed-ultra d-block no-underline link-gray-dark no-underline h5 Bump-link--hover\"  data-ga-click=\"(Logged out) Header, go to Education\">Education <span class=\"Bump-link-symbol float-right text-normal text-gray-light\">&rarr;</span></a></li>\n",
      "                    </ul>\n",
      "                  </div>\n",
      "                </details>\n",
      "              </li>\n",
      "          </ul>\n",
      "        </nav>\n",
      "\n",
      "      <div class=\"d-lg-flex flex-items-center px-3 px-lg-0 text-center text-lg-left\">\n",
      "          <div class=\"d-lg-flex mb-3 mb-lg-0\">\n",
      "            <div class=\"header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to\"\n",
      "  role=\"combobox\"\n",
      "  aria-owns=\"jump-to-results\"\n",
      "  aria-label=\"Search or jump to\"\n",
      "  aria-haspopup=\"listbox\"\n",
      "  aria-expanded=\"false\"\n",
      ">\n",
      "  <div class=\"position-relative\">\n",
      "    <!-- '\"` --><!-- </textarea></xmp> --></option></form><form class=\"js-site-search-form\" role=\"search\" aria-label=\"Site\" data-flagged-in=\"false\" data-scope-type=\"Repository\" data-scope-id=\"142071553\" data-scoped-search-url=\"/splicemachine/pysplice/search\" data-owner-scoped-search-url=\"/orgs/splicemachine/search\" data-unscoped-search-url=\"/search\" action=\"/splicemachine/pysplice/search\" accept-charset=\"UTF-8\" method=\"get\">\n",
      "      <label class=\"form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center\">\n",
      "        <input type=\"text\"\n",
      "          class=\"form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable\"\n",
      "          data-hotkey=\"s,/\"\n",
      "          name=\"q\"\n",
      "          value=\"\"\n",
      "          placeholder=\"Search\"\n",
      "          data-unscoped-placeholder=\"Search GitHub\"\n",
      "          data-scoped-placeholder=\"Search\"\n",
      "          autocapitalize=\"off\"\n",
      "          aria-autocomplete=\"list\"\n",
      "          aria-controls=\"jump-to-results\"\n",
      "          aria-label=\"Search\"\n",
      "          data-jump-to-suggestions-path=\"/_graphql/GetSuggestedNavigationDestinations\"\n",
      "          spellcheck=\"false\"\n",
      "          autocomplete=\"off\"\n",
      "          >\n",
      "          <input type=\"hidden\" data-csrf=\"true\" class=\"js-data-jump-to-suggestions-path-csrf\" value=\"Zp7IUGO/tYXCVQu3obNdgD+NXdUIJMvaBg4w1JGJVtko+GA28p5piHlrtL9c1S4y/yLRWkPz9osSOzDsax0ErQ==\" />\n",
      "          <input type=\"hidden\" class=\"js-site-search-type-field\" name=\"type\" >\n",
      "            <img src=\"https://github.githubassets.com/images/search-key-slash.svg\" alt=\"\" class=\"mr-2 header-search-key-slash\">\n",
      "\n",
      "            <div class=\"Box position-absolute overflow-hidden d-none jump-to-suggestions js-jump-to-suggestions-container\">\n",
      "              \n",
      "<ul class=\"d-none js-jump-to-suggestions-template-container\">\n",
      "  \n",
      "\n",
      "<li class=\"d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-suggestion\" role=\"option\">\n",
      "  <a tabindex=\"-1\" class=\"no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2\" href=\"\" data-item-type=\"suggestion\">\n",
      "    <div class=\"jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none\">\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none\" title=\"Repository\" aria-label=\"Repository\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none\" title=\"Project\" aria-label=\"Project\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M1.75 0A1.75 1.75 0 000 1.75v12.5C0 15.216.784 16 1.75 16h12.5A1.75 1.75 0 0016 14.25V1.75A1.75 1.75 0 0014.25 0H1.75zM1.5 1.75a.25.25 0 01.25-.25h12.5a.25.25 0 01.25.25v12.5a.25.25 0 01-.25.25H1.75a.25.25 0 01-.25-.25V1.75zM11.75 3a.75.75 0 00-.75.75v7.5a.75.75 0 001.5 0v-7.5a.75.75 0 00-.75-.75zm-8.25.75a.75.75 0 011.5 0v5.5a.75.75 0 01-1.5 0v-5.5zM8 3a.75.75 0 00-.75.75v3.5a.75.75 0 001.5 0v-3.5A.75.75 0 008 3z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none\" title=\"Search\" aria-label=\"Search\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z\"></path></svg>\n",
      "    </div>\n",
      "\n",
      "    <img class=\"avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none\" alt=\"\" aria-label=\"Team\" src=\"\" width=\"28\" height=\"28\">\n",
      "\n",
      "    <div class=\"jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target\">\n",
      "    </div>\n",
      "\n",
      "    <div class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search\">\n",
      "      <span class=\"js-jump-to-badge-search-text-default d-none\" aria-label=\"in this repository\">\n",
      "        In this repository\n",
      "      </span>\n",
      "      <span class=\"js-jump-to-badge-search-text-global d-none\" aria-label=\"in all of GitHub\">\n",
      "        All GitHub\n",
      "      </span>\n",
      "      <span aria-hidden=\"true\" class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "\n",
      "    <div aria-hidden=\"true\" class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump\">\n",
      "      Jump to\n",
      "      <span class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "  </a>\n",
      "</li>\n",
      "\n",
      "</ul>\n",
      "\n",
      "<ul class=\"d-none js-jump-to-no-results-template-container\">\n",
      "  <li class=\"d-flex flex-justify-center flex-items-center f5 d-none js-jump-to-suggestion p-2\">\n",
      "    <span class=\"text-gray\">No suggested jump to results</span>\n",
      "  </li>\n",
      "</ul>\n",
      "\n",
      "<ul id=\"jump-to-results\" role=\"listbox\" class=\"p-0 m-0 js-navigation-container jump-to-suggestions-results-container js-jump-to-suggestions-results-container\">\n",
      "  \n",
      "\n",
      "<li class=\"d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-scoped-search d-none\" role=\"option\">\n",
      "  <a tabindex=\"-1\" class=\"no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2\" href=\"\" data-item-type=\"scoped_search\">\n",
      "    <div class=\"jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none\">\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none\" title=\"Repository\" aria-label=\"Repository\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none\" title=\"Project\" aria-label=\"Project\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M1.75 0A1.75 1.75 0 000 1.75v12.5C0 15.216.784 16 1.75 16h12.5A1.75 1.75 0 0016 14.25V1.75A1.75 1.75 0 0014.25 0H1.75zM1.5 1.75a.25.25 0 01.25-.25h12.5a.25.25 0 01.25.25v12.5a.25.25 0 01-.25.25H1.75a.25.25 0 01-.25-.25V1.75zM11.75 3a.75.75 0 00-.75.75v7.5a.75.75 0 001.5 0v-7.5a.75.75 0 00-.75-.75zm-8.25.75a.75.75 0 011.5 0v5.5a.75.75 0 01-1.5 0v-5.5zM8 3a.75.75 0 00-.75.75v3.5a.75.75 0 001.5 0v-3.5A.75.75 0 008 3z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none\" title=\"Search\" aria-label=\"Search\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z\"></path></svg>\n",
      "    </div>\n",
      "\n",
      "    <img class=\"avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none\" alt=\"\" aria-label=\"Team\" src=\"\" width=\"28\" height=\"28\">\n",
      "\n",
      "    <div class=\"jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target\">\n",
      "    </div>\n",
      "\n",
      "    <div class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search\">\n",
      "      <span class=\"js-jump-to-badge-search-text-default d-none\" aria-label=\"in this repository\">\n",
      "        In this repository\n",
      "      </span>\n",
      "      <span class=\"js-jump-to-badge-search-text-global d-none\" aria-label=\"in all of GitHub\">\n",
      "        All GitHub\n",
      "      </span>\n",
      "      <span aria-hidden=\"true\" class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "\n",
      "    <div aria-hidden=\"true\" class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump\">\n",
      "      Jump to\n",
      "      <span class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "  </a>\n",
      "</li>\n",
      "\n",
      "  \n",
      "\n",
      "<li class=\"d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item js-jump-to-global-search d-none\" role=\"option\">\n",
      "  <a tabindex=\"-1\" class=\"no-underline d-flex flex-auto flex-items-center jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open p-2\" href=\"\" data-item-type=\"global_search\">\n",
      "    <div class=\"jump-to-octicon js-jump-to-octicon flex-shrink-0 mr-2 text-center d-none\">\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-repo flex-shrink-0 js-jump-to-octicon-repo d-none\" title=\"Repository\" aria-label=\"Repository\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-project flex-shrink-0 js-jump-to-octicon-project d-none\" title=\"Project\" aria-label=\"Project\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M1.75 0A1.75 1.75 0 000 1.75v12.5C0 15.216.784 16 1.75 16h12.5A1.75 1.75 0 0016 14.25V1.75A1.75 1.75 0 0014.25 0H1.75zM1.5 1.75a.25.25 0 01.25-.25h12.5a.25.25 0 01.25.25v12.5a.25.25 0 01-.25.25H1.75a.25.25 0 01-.25-.25V1.75zM11.75 3a.75.75 0 00-.75.75v7.5a.75.75 0 001.5 0v-7.5a.75.75 0 00-.75-.75zm-8.25.75a.75.75 0 011.5 0v5.5a.75.75 0 01-1.5 0v-5.5zM8 3a.75.75 0 00-.75.75v3.5a.75.75 0 001.5 0v-3.5A.75.75 0 008 3z\"></path></svg>\n",
      "      <svg height=\"16\" width=\"16\" class=\"octicon octicon-search flex-shrink-0 js-jump-to-octicon-search d-none\" title=\"Search\" aria-label=\"Search\" viewBox=\"0 0 16 16\" version=\"1.1\" role=\"img\"><path fill-rule=\"evenodd\" d=\"M11.5 7a4.499 4.499 0 11-8.998 0A4.499 4.499 0 0111.5 7zm-.82 4.74a6 6 0 111.06-1.06l3.04 3.04a.75.75 0 11-1.06 1.06l-3.04-3.04z\"></path></svg>\n",
      "    </div>\n",
      "\n",
      "    <img class=\"avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar d-none\" alt=\"\" aria-label=\"Team\" src=\"\" width=\"28\" height=\"28\">\n",
      "\n",
      "    <div class=\"jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target\">\n",
      "    </div>\n",
      "\n",
      "    <div class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search\">\n",
      "      <span class=\"js-jump-to-badge-search-text-default d-none\" aria-label=\"in this repository\">\n",
      "        In this repository\n",
      "      </span>\n",
      "      <span class=\"js-jump-to-badge-search-text-global d-none\" aria-label=\"in all of GitHub\">\n",
      "        All GitHub\n",
      "      </span>\n",
      "      <span aria-hidden=\"true\" class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "\n",
      "    <div aria-hidden=\"true\" class=\"border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump\">\n",
      "      Jump to\n",
      "      <span class=\"d-inline-block ml-1 v-align-middle\">↵</span>\n",
      "    </div>\n",
      "  </a>\n",
      "</li>\n",
      "\n",
      "\n",
      "</ul>\n",
      "\n",
      "            </div>\n",
      "      </label>\n",
      "</form>  </div>\n",
      "</div>\n",
      "\n",
      "          </div>\n",
      "\n",
      "        <a href=\"/login?return_to=%2Fsplicemachine%2Fpysplice%2Fblob%2Fmaster%2FREADME.md\"\n",
      "          class=\"HeaderMenu-link no-underline mr-3\"\n",
      "          data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"5de8abcbf87ffd82a62d6e91ae8b9368e2e136a47a8a338a6662951d02aea8b8\"\n",
      "          data-ga-click=\"(Logged out) Header, clicked Sign in, text:sign-in\">\n",
      "          Sign&nbsp;in\n",
      "        </a>\n",
      "            <a href=\"/join?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=splicemachine%2Fpysplice\"\n",
      "              class=\"HeaderMenu-link d-inline-block no-underline border border-gray-dark rounded-1 px-2 py-1\"\n",
      "              data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"5de8abcbf87ffd82a62d6e91ae8b9368e2e136a47a8a338a6662951d02aea8b8\"\n",
      "              data-ga-click=\"Sign up, click to sign up for account, ref_page:/&lt;user-name&gt;/&lt;repo-name&gt;/blob/show;ref_cta:Sign up;ref_loc:header logged out\">\n",
      "              Sign&nbsp;up\n",
      "            </a>\n",
      "      </div>\n",
      "    </div>\n",
      "  </div>\n",
      "</header>\n",
      "\n",
      "    </div>\n",
      "\n",
      "  <div id=\"start-of-content\" class=\"show-on-focus\"></div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    <div data-pjax-replace id=\"js-flash-container\">\n",
      "\n",
      "\n",
      "  <template class=\"js-flash-template\">\n",
      "    <div class=\"flash flash-full  {{ className }}\">\n",
      "  <div class=\" px-2\" >\n",
      "    <button class=\"flash-close js-flash-close\" type=\"button\" aria-label=\"Dismiss this message\">\n",
      "      <svg class=\"octicon octicon-x\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M3.72 3.72a.75.75 0 011.06 0L8 6.94l3.22-3.22a.75.75 0 111.06 1.06L9.06 8l3.22 3.22a.75.75 0 11-1.06 1.06L8 9.06l-3.22 3.22a.75.75 0 01-1.06-1.06L6.94 8 3.72 4.78a.75.75 0 010-1.06z\"></path></svg>\n",
      "    </button>\n",
      "    \n",
      "      <div>{{ message }}</div>\n",
      "\n",
      "  </div>\n",
      "</div>\n",
      "  </template>\n",
      "</div>\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "  <include-fragment class=\"js-notification-shelf-include-fragment\" data-base-src=\"https://github.com/notifications/beta/shelf\"></include-fragment>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  <div\n",
      "    class=\"application-main \"\n",
      "    data-commit-hovercards-enabled\n",
      "    data-discussion-hovercards-enabled\n",
      "    data-issue-and-pr-hovercards-enabled\n",
      "  >\n",
      "        <div itemscope itemtype=\"http://schema.org/SoftwareSourceCode\" class=\"\">\n",
      "    <main id=\"js-repo-pjax-container\" data-pjax-container >\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  <div class=\"bg-gray-light pt-3 hide-full-screen mb-5\">\n",
      "\n",
      "      <div class=\"d-flex mb-3 px-3 px-md-4 px-lg-5\">\n",
      "\n",
      "        <div class=\"flex-auto min-width-0 width-fit mr-3\">\n",
      "            <h1 class=\" d-flex flex-wrap flex-items-center break-word f3 text-normal\">\n",
      "    <svg class=\"octicon octicon-repo text-gray mr-2\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M2 2.5A2.5 2.5 0 014.5 0h8.75a.75.75 0 01.75.75v12.5a.75.75 0 01-.75.75h-2.5a.75.75 0 110-1.5h1.75v-2h-8a1 1 0 00-.714 1.7.75.75 0 01-1.072 1.05A2.495 2.495 0 012 11.5v-9zm10.5-1V9h-8c-.356 0-.694.074-1 .208V2.5a1 1 0 011-1h8zM5 12.25v3.25a.25.25 0 00.4.2l1.45-1.087a.25.25 0 01.3 0L8.6 15.7a.25.25 0 00.4-.2v-3.25a.25.25 0 00-.25-.25h-3.5a.25.25 0 00-.25.25z\"></path></svg>\n",
      "    <span class=\"author flex-self-stretch\" itemprop=\"author\">\n",
      "      <a class=\"url fn\" rel=\"author\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/splicemachine/hovercard\" href=\"/splicemachine\">splicemachine</a>\n",
      "    </span>\n",
      "    <span class=\"mx-1 flex-self-stretch color-text-secondary\">/</span>\n",
      "  <strong itemprop=\"name\" class=\"mr-2 flex-self-stretch\">\n",
      "    <a data-pjax=\"#js-repo-pjax-container\" class=\"\" href=\"/splicemachine/pysplice\">pysplice</a>\n",
      "  </strong>\n",
      "  \n",
      "</h1>\n",
      "\n",
      "\n",
      "        </div>\n",
      "\n",
      "          <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\" style=\"padding: 2px 0;\">\n",
      "\n",
      "  <li>\n",
      "          <a class=\"tooltipped tooltipped-s btn btn-sm btn-with-count\" aria-label=\"You must be signed in to watch a repository\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"eeffb5e9acd111b40d380c05fe196ede60d2e5334338dffe4bd3ed3fc078f80f\" href=\"/login?return_to=%2Fsplicemachine%2Fpysplice\">\n",
      "    <svg class=\"octicon octicon-eye\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M1.679 7.932c.412-.621 1.242-1.75 2.366-2.717C5.175 4.242 6.527 3.5 8 3.5c1.473 0 2.824.742 3.955 1.715 1.124.967 1.954 2.096 2.366 2.717a.119.119 0 010 .136c-.412.621-1.242 1.75-2.366 2.717C10.825 11.758 9.473 12.5 8 12.5c-1.473 0-2.824-.742-3.955-1.715C2.92 9.818 2.09 8.69 1.679 8.068a.119.119 0 010-.136zM8 2c-1.981 0-3.67.992-4.933 2.078C1.797 5.169.88 6.423.43 7.1a1.619 1.619 0 000 1.798c.45.678 1.367 1.932 2.637 3.024C4.329 13.008 6.019 14 8 14c1.981 0 3.67-.992 4.933-2.078 1.27-1.091 2.187-2.345 2.637-3.023a1.619 1.619 0 000-1.798c-.45-.678-1.367-1.932-2.637-3.023C11.671 2.992 9.981 2 8 2zm0 8a2 2 0 100-4 2 2 0 000 4z\"></path></svg>\n",
      "    Watch\n",
      "</a>    <a class=\"social-count\" href=\"/splicemachine/pysplice/watchers\"\n",
      "       aria-label=\"26 users are watching this repository\">\n",
      "      26\n",
      "    </a>\n",
      "\n",
      "  </li>\n",
      "\n",
      "  <li>\n",
      "          <a class=\"btn btn-sm btn-with-count  tooltipped tooltipped-s\" aria-label=\"You must be signed in to star a repository\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:142071553,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"b1b802cfce90d1f40112ab3cd465a420c726cea37a6b32abf37234e1f65a1f1d\" href=\"/login?return_to=%2Fsplicemachine%2Fpysplice\">\n",
      "      <svg class=\"octicon octicon-star v-align-text-bottom mr-1\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8 .25a.75.75 0 01.673.418l1.882 3.815 4.21.612a.75.75 0 01.416 1.279l-3.046 2.97.719 4.192a.75.75 0 01-1.088.791L8 12.347l-3.766 1.98a.75.75 0 01-1.088-.79l.72-4.194L.818 6.374a.75.75 0 01.416-1.28l4.21-.611L7.327.668A.75.75 0 018 .25zm0 2.445L6.615 5.5a.75.75 0 01-.564.41l-3.097.45 2.24 2.184a.75.75 0 01.216.664l-.528 3.084 2.769-1.456a.75.75 0 01.698 0l2.77 1.456-.53-3.084a.75.75 0 01.216-.664l2.24-2.183-3.096-.45a.75.75 0 01-.564-.41L8 2.694v.001z\"></path></svg>\n",
      "      <span>\n",
      "        Star\n",
      "</span></a>\n",
      "    <a class=\"social-count js-social-count\" href=\"/splicemachine/pysplice/stargazers\"\n",
      "      aria-label=\"7 users starred this repository\">\n",
      "      7\n",
      "    </a>\n",
      "\n",
      "  </li>\n",
      "\n",
      "  <li>\n",
      "        <a class=\"btn btn-sm btn-with-count tooltipped tooltipped-s\" aria-label=\"You must be signed in to fork a repository\" rel=\"nofollow\" data-hydro-click=\"{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:142071553,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"a48ace751feefc2c9cf1bae3b2a790ad27108c913093e8caae142604d384bd4a\" href=\"/login?return_to=%2Fsplicemachine%2Fpysplice\">\n",
      "          <svg class=\"octicon octicon-repo-forked\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M5 3.25a.75.75 0 11-1.5 0 .75.75 0 011.5 0zm0 2.122a2.25 2.25 0 10-1.5 0v.878A2.25 2.25 0 005.75 8.5h1.5v2.128a2.251 2.251 0 101.5 0V8.5h1.5a2.25 2.25 0 002.25-2.25v-.878a2.25 2.25 0 10-1.5 0v.878a.75.75 0 01-.75.75h-4.5A.75.75 0 015 6.25v-.878zm3.75 7.378a.75.75 0 11-1.5 0 .75.75 0 011.5 0zm3-8.75a.75.75 0 100-1.5.75.75 0 000 1.5z\"></path></svg>\n",
      "          Fork\n",
      "</a>\n",
      "      <a href=\"/splicemachine/pysplice/network/members\" class=\"social-count\"\n",
      "         aria-label=\"2 users forked this repository\">\n",
      "        2\n",
      "      </a>\n",
      "  </li>\n",
      "</ul>\n",
      "\n",
      "      </div>\n",
      "        \n",
      "<nav aria-label=\"Repository\" data-pjax=\"#js-repo-pjax-container\" class=\"js-repo-nav js-sidenav-container-pjax js-responsive-underlinenav overflow-hidden UnderlineNav px-3 px-md-4 px-lg-5 bg-gray-light\">\n",
      "  <ul class=\"UnderlineNav-body list-style-none \">\n",
      "          <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item selected UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i0code-tab\" data-hotkey=\"g c\" data-ga-click=\"Repository, Navigation click, Code tab\" aria-current=\"page\" data-selected-links=\"repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments /splicemachine/pysplice\" href=\"/splicemachine/pysplice\">\n",
      "              <svg class=\"octicon octicon-code UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\"></path></svg>\n",
      "            <span data-content=\"Code\">Code</span>\n",
      "              <span title=\"Not available\" class=\"Counter \"></span>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "      <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i1issues-tab\" data-hotkey=\"g i\" data-ga-click=\"Repository, Navigation click, Issues tab\" data-selected-links=\"repo_issues repo_labels repo_milestones /splicemachine/pysplice/issues\" href=\"/splicemachine/pysplice/issues\">\n",
      "              <svg class=\"octicon octicon-issue-opened UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm9 3a1 1 0 11-2 0 1 1 0 012 0zm-.25-6.25a.75.75 0 00-1.5 0v3.5a.75.75 0 001.5 0v-3.5z\"></path></svg>\n",
      "            <span data-content=\"Issues\">Issues</span>\n",
      "              <span title=\"1\" class=\"Counter \">1</span>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "      <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i2pull-requests-tab\" data-hotkey=\"g p\" data-ga-click=\"Repository, Navigation click, Pull requests tab\" data-selected-links=\"repo_pulls checks /splicemachine/pysplice/pulls\" href=\"/splicemachine/pysplice/pulls\">\n",
      "              <svg class=\"octicon octicon-git-pull-request UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.177 3.073L9.573.677A.25.25 0 0110 .854v4.792a.25.25 0 01-.427.177L7.177 3.427a.25.25 0 010-.354zM3.75 2.5a.75.75 0 100 1.5.75.75 0 000-1.5zm-2.25.75a2.25 2.25 0 113 2.122v5.256a2.251 2.251 0 11-1.5 0V5.372A2.25 2.25 0 011.5 3.25zM11 2.5h-1V4h1a1 1 0 011 1v5.628a2.251 2.251 0 101.5 0V5A2.5 2.5 0 0011 2.5zm1 10.25a.75.75 0 111.5 0 .75.75 0 01-1.5 0zM3.75 12a.75.75 0 100 1.5.75.75 0 000-1.5z\"></path></svg>\n",
      "            <span data-content=\"Pull requests\">Pull requests</span>\n",
      "              <span title=\"4\" class=\"Counter \">4</span>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "      <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i3actions-tab\" data-hotkey=\"g a\" data-ga-click=\"Repository, Navigation click, Actions tab\" data-selected-links=\"repo_actions /splicemachine/pysplice/actions\" href=\"/splicemachine/pysplice/actions\">\n",
      "              <svg class=\"octicon octicon-play UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M1.5 8a6.5 6.5 0 1113 0 6.5 6.5 0 01-13 0zM8 0a8 8 0 100 16A8 8 0 008 0zM6.379 5.227A.25.25 0 006 5.442v5.117a.25.25 0 00.379.214l4.264-2.559a.25.25 0 000-.428L6.379 5.227z\"></path></svg>\n",
      "            <span data-content=\"Actions\">Actions</span>\n",
      "              <span title=\"Not available\" class=\"Counter \"></span>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "      <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i4security-tab\" data-hotkey=\"g s\" data-ga-click=\"Repository, Navigation click, Security tab\" data-selected-links=\"security overview alerts policy token_scanning code_scanning /splicemachine/pysplice/security\" href=\"/splicemachine/pysplice/security\">\n",
      "              <svg class=\"octicon octicon-shield UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.467.133a1.75 1.75 0 011.066 0l5.25 1.68A1.75 1.75 0 0115 3.48V7c0 1.566-.32 3.182-1.303 4.682-.983 1.498-2.585 2.813-5.032 3.855a1.7 1.7 0 01-1.33 0c-2.447-1.042-4.049-2.357-5.032-3.855C1.32 10.182 1 8.566 1 7V3.48a1.75 1.75 0 011.217-1.667l5.25-1.68zm.61 1.429a.25.25 0 00-.153 0l-5.25 1.68a.25.25 0 00-.174.238V7c0 1.358.275 2.666 1.057 3.86.784 1.194 2.121 2.34 4.366 3.297a.2.2 0 00.154 0c2.245-.956 3.582-2.104 4.366-3.298C13.225 9.666 13.5 8.36 13.5 7V3.48a.25.25 0 00-.174-.237l-5.25-1.68zM9 10.5a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.75a.75.75 0 10-1.5 0v3a.75.75 0 001.5 0v-3z\"></path></svg>\n",
      "            <span data-content=\"Security\">Security</span>\n",
      "              <include-fragment src=\"/splicemachine/pysplice/security/overall-count\" accept=\"text/fragment+html\"></include-fragment>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "      <li class=\"d-flex\">\n",
      "        <a class=\"js-selected-navigation-item UnderlineNav-item hx_underlinenav-item no-wrap js-responsive-underlinenav-item\" data-tab-item=\"i5insights-tab\" data-ga-click=\"Repository, Navigation click, Insights tab\" data-selected-links=\"repo_graphs repo_contributors dependency_graph dependabot_updates pulse people /splicemachine/pysplice/pulse\" href=\"/splicemachine/pysplice/pulse\">\n",
      "              <svg class=\"octicon octicon-graph UnderlineNav-octicon d-none d-sm-inline\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M1.5 1.75a.75.75 0 00-1.5 0v12.5c0 .414.336.75.75.75h14.5a.75.75 0 000-1.5H1.5V1.75zm14.28 2.53a.75.75 0 00-1.06-1.06L10 7.94 7.53 5.47a.75.75 0 00-1.06 0L3.22 8.72a.75.75 0 001.06 1.06L7 7.06l2.47 2.47a.75.75 0 001.06 0l5.25-5.25z\"></path></svg>\n",
      "            <span data-content=\"Insights\">Insights</span>\n",
      "              <span title=\"Not available\" class=\"Counter \"></span>\n",
      "</a>\n",
      "\n",
      "      </li>\n",
      "\n",
      "</ul>        <div class=\"position-absolute right-0 pr-3 pr-md-4 pr-lg-5 js-responsive-underlinenav-overflow\" style=\"visibility:hidden;\">\n",
      "      <details class=\"details-overlay details-reset position-relative\">\n",
      "  <summary role=\"button\">\n",
      "    <div class=\"UnderlineNav-item mr-0 border-0\">\n",
      "            <svg class=\"octicon octicon-kebab-horizontal\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"M8 9a1.5 1.5 0 100-3 1.5 1.5 0 000 3zM1.5 9a1.5 1.5 0 100-3 1.5 1.5 0 000 3zm13 0a1.5 1.5 0 100-3 1.5 1.5 0 000 3z\"></path></svg>\n",
      "            <span class=\"sr-only\">More</span>\n",
      "          </div>\n",
      "</summary>  <div>\n",
      "    <details-menu role=\"menu\" class=\"dropdown-menu dropdown-menu-sw \">\n",
      "  \n",
      "            <ul>\n",
      "                <li data-menu-item=\"i0code-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice\" href=\"/splicemachine/pysplice\">\n",
      "                    Code\n",
      "</a>                </li>\n",
      "                <li data-menu-item=\"i1issues-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice/issues\" href=\"/splicemachine/pysplice/issues\">\n",
      "                    Issues\n",
      "</a>                </li>\n",
      "                <li data-menu-item=\"i2pull-requests-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice/pulls\" href=\"/splicemachine/pysplice/pulls\">\n",
      "                    Pull requests\n",
      "</a>                </li>\n",
      "                <li data-menu-item=\"i3actions-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice/actions\" href=\"/splicemachine/pysplice/actions\">\n",
      "                    Actions\n",
      "</a>                </li>\n",
      "                <li data-menu-item=\"i4security-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice/security\" href=\"/splicemachine/pysplice/security\">\n",
      "                    Security\n",
      "</a>                </li>\n",
      "                <li data-menu-item=\"i5insights-tab\" hidden>\n",
      "                  <a role=\"menuitem\" class=\"js-selected-navigation-item dropdown-item\" data-selected-links=\" /splicemachine/pysplice/pulse\" href=\"/splicemachine/pysplice/pulse\">\n",
      "                    Insights\n",
      "</a>                </li>\n",
      "            </ul>\n",
      "\n",
      "</details-menu>\n",
      "</div></details>    </div>\n",
      "\n",
      "</nav>\n",
      "  </div>\n",
      "\n",
      "\n",
      "<div class=\"container-xl clearfix new-discussion-timeline px-3 px-md-4 px-lg-5\">\n",
      "  <div class=\"repository-content \" >\n",
      "\n",
      "    \n",
      "      \n",
      "  \n",
      "\n",
      "\n",
      "    <a class=\"d-none js-permalink-shortcut\" data-hotkey=\"y\" href=\"/splicemachine/pysplice/blob/95d4d170aeafd145955a532bb972068f3b93f341/README.md\">Permalink</a>\n",
      "\n",
      "    <!-- blob contrib key: blob_contributors:v22:9ad0d1fbbc9ec2213c36b541dc1672ae8f339879fecea802c9498ed73ef2d222 -->\n",
      "\n",
      "    <div class=\"d-flex flex-items-start flex-shrink-0 pb-3 flex-wrap flex-md-nowrap flex-justify-between flex-md-justify-start\">\n",
      "      \n",
      "<div class=\"position-relative\">\n",
      "  <details class=\"details-reset details-overlay mr-0 mb-0 \" id=\"branch-select-menu\">\n",
      "    <summary class=\"btn css-truncate\"\n",
      "            data-hotkey=\"w\"\n",
      "            title=\"Switch branches or tags\">\n",
      "      <svg class=\"octicon octicon-git-branch text-gray\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M11.75 2.5a.75.75 0 100 1.5.75.75 0 000-1.5zm-2.25.75a2.25 2.25 0 113 2.122V6A2.5 2.5 0 0110 8.5H6a1 1 0 00-1 1v1.128a2.251 2.251 0 11-1.5 0V5.372a2.25 2.25 0 111.5 0v1.836A2.492 2.492 0 016 7h4a1 1 0 001-1v-.628A2.25 2.25 0 019.5 3.25zM4.25 12a.75.75 0 100 1.5.75.75 0 000-1.5zM3.5 3.25a.75.75 0 111.5 0 .75.75 0 01-1.5 0z\"></path></svg>\n",
      "      <span class=\"css-truncate-target\" data-menu-button>master</span>\n",
      "      <span class=\"dropdown-caret\"></span>\n",
      "    </summary>\n",
      "\n",
      "    <details-menu class=\"SelectMenu SelectMenu--hasFilter\" src=\"/splicemachine/pysplice/refs/master/README.md?source_action=show&amp;source_controller=blob\" preload>\n",
      "      <div class=\"SelectMenu-modal\">\n",
      "        <include-fragment class=\"SelectMenu-loading\" aria-label=\"Menu is loading\">\n",
      "          <svg class=\"octicon octicon-octoface anim-pulse\" height=\"32\" viewBox=\"0 0 24 24\" version=\"1.1\" width=\"32\" aria-hidden=\"true\"><path d=\"M7.75 11c-.69 0-1.25.56-1.25 1.25v1.5a1.25 1.25 0 102.5 0v-1.5C9 11.56 8.44 11 7.75 11zm1.27 4.5a.469.469 0 01.48-.5h5a.47.47 0 01.48.5c-.116 1.316-.759 2.5-2.98 2.5s-2.864-1.184-2.98-2.5zm7.23-4.5c-.69 0-1.25.56-1.25 1.25v1.5a1.25 1.25 0 102.5 0v-1.5c0-.69-.56-1.25-1.25-1.25z\"></path><path fill-rule=\"evenodd\" d=\"M21.255 3.82a1.725 1.725 0 00-2.141-1.195c-.557.16-1.406.44-2.264.866-.78.386-1.647.93-2.293 1.677A18.442 18.442 0 0012 5c-.93 0-1.784.059-2.569.17-.645-.74-1.505-1.28-2.28-1.664a13.876 13.876 0 00-2.265-.866 1.725 1.725 0 00-2.141 1.196 23.645 23.645 0 00-.69 3.292c-.125.97-.191 2.07-.066 3.112C1.254 11.882 1 13.734 1 15.527 1 19.915 3.13 23 12 23c8.87 0 11-3.053 11-7.473 0-1.794-.255-3.647-.99-5.29.127-1.046.06-2.15-.066-3.125a23.652 23.652 0 00-.689-3.292zM20.5 14c.5 3.5-1.5 6.5-8.5 6.5s-9-3-8.5-6.5c.583-4 3-6 8.5-6s7.928 2 8.5 6z\"></path></svg>\n",
      "        </include-fragment>\n",
      "      </div>\n",
      "    </details-menu>\n",
      "  </details>\n",
      "\n",
      "</div>\n",
      "\n",
      "      <h2 id=\"blob-path\" class=\"breadcrumb flex-auto min-width-0 text-normal mx-0 mx-md-3 width-full width-md-auto flex-order-1 flex-md-order-none mt-3 mt-md-0\">\n",
      "        <span class=\"js-repo-root text-bold\"><span class=\"js-path-segment d-inline-block wb-break-all\"><a data-pjax=\"true\" href=\"/splicemachine/pysplice\"><span>pysplice</span></a></span></span><span class=\"separator\">/</span><strong class=\"final-path\">README.md</strong>\n",
      "      </h2>\n",
      "      <a href=\"/splicemachine/pysplice/find/master\"\n",
      "            class=\"js-pjax-capture-input btn mr-2 d-none d-md-block\"\n",
      "            data-pjax\n",
      "            data-hotkey=\"t\">\n",
      "        Go to file\n",
      "      </a>\n",
      "\n",
      "      <details id=\"blob-more-options-details\" class=\"details-overlay details-reset position-relative\">\n",
      "  <summary role=\"button\" type=\"button\" class=\"btn \">\n",
      "    <svg aria-label=\"More options\" class=\"octicon octicon-kebab-horizontal\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" role=\"img\"><path d=\"M8 9a1.5 1.5 0 100-3 1.5 1.5 0 000 3zM1.5 9a1.5 1.5 0 100-3 1.5 1.5 0 000 3zm13 0a1.5 1.5 0 100-3 1.5 1.5 0 000 3z\"></path></svg>\n",
      "</summary>  <div>\n",
      "    <ul class=\"dropdown-menu dropdown-menu-sw\">\n",
      "            <li class=\"d-block d-md-none\">\n",
      "              <a class=\"dropdown-item d-flex flex-items-baseline\" data-hydro-click=\"{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;target&quot;:&quot;FIND_FILE_BUTTON&quot;,&quot;repository_id&quot;:142071553,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\" data-hydro-click-hmac=\"70a8c85c5cda396036f066a1b4232fb8537e5a6732f29cf6bace7f6cb604b840\" data-ga-click=\"Repository, find file, location:repo overview\" data-hotkey=\"t\" data-pjax=\"true\" href=\"/splicemachine/pysplice/find/master\">\n",
      "                <span class=\"flex-auto\">Go to file</span>\n",
      "                <span class=\"text-small text-gray\" aria-hidden=\"true\">T</span>\n",
      "</a>            </li>\n",
      "            <li data-toggle-for=\"blob-more-options-details\">\n",
      "              <button type=\"button\" data-toggle-for=\"jumpto-line-details-dialog\" class=\"btn-link dropdown-item\">\n",
      "                <span class=\"d-flex flex-items-baseline\">\n",
      "                  <span class=\"flex-auto\">Go to line</span>\n",
      "                  <span class=\"text-small text-gray\" aria-hidden=\"true\">L</span>\n",
      "                </span>\n",
      "              </button>\n",
      "            </li>\n",
      "            <li class=\"dropdown-divider\" role=\"none\"></li>\n",
      "            <li>\n",
      "              <clipboard-copy value=\"README.md\" class=\"dropdown-item cursor-pointer\" data-toggle-for=\"blob-more-options-details\">\n",
      "                Copy path\n",
      "              </clipboard-copy>\n",
      "            </li>\n",
      "          </ul>\n",
      "</div></details>    </div>\n",
      "\n",
      "\n",
      "\n",
      "    <div class=\"Box d-flex flex-column flex-shrink-0 mb-3\">\n",
      "      <include-fragment src=\"/splicemachine/pysplice/contributors/master/README.md\" class=\"commit-loader\">\n",
      "        <div class=\"Box-header Box-header--blue d-flex flex-items-center\">\n",
      "          <div class=\"Skeleton avatar avatar-user flex-shrink-0 ml-n1 mr-n1 mt-n1 mb-n1\" style=\"width:24px;height:24px;\"></div>\n",
      "          <div class=\"Skeleton Skeleton--text col-5 ml-2\">&nbsp;</div>\n",
      "        </div>\n",
      "\n",
      "        <div class=\"Box-body d-flex flex-items-center\" >\n",
      "          <div class=\"Skeleton Skeleton--text col-1\">&nbsp;</div>\n",
      "          <span class=\"text-red h6 loader-error\">Cannot retrieve contributors at this time</span>\n",
      "        </div>\n",
      "</include-fragment>    </div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    <div class=\"Box mt-3 position-relative\n",
      "      \">\n",
      "      \n",
      "<div class=\"Box-header py-2 d-flex flex-column flex-shrink-0 flex-md-row flex-md-items-center\">\n",
      "  <div class=\"text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0\">\n",
      "      <include-fragment src=\"/splicemachine/pysplice/codeowners/master/README.md\"></include-fragment>\n",
      "\n",
      "      1309 lines (1263 sloc)\n",
      "      <span class=\"file-info-divider\"></span>\n",
      "    63.4 KB\n",
      "  </div>\n",
      "\n",
      "  <div class=\"d-flex py-1 py-md-0 flex-auto flex-order-1 flex-md-order-2 flex-sm-grow-0 flex-justify-between\">\n",
      "\n",
      "    <div class=\"BtnGroup\">\n",
      "      <a href=\"/splicemachine/pysplice/raw/master/README.md\" id=\"raw-url\" role=\"button\" class=\"btn btn-sm BtnGroup-item \">Raw</a>\n",
      "        <a href=\"/splicemachine/pysplice/blame/master/README.md\" data-hotkey=\"b\" role=\"button\" class=\"btn js-update-url-with-hash btn-sm BtnGroup-item \">Blame</a>\n",
      "    </div>\n",
      "\n",
      "    <div>\n",
      "          <a class=\"btn-octicon tooltipped tooltipped-nw js-remove-unless-platform\"\n",
      "             data-platforms=\"windows,mac\"\n",
      "             href=\"https://desktop.github.com\"\n",
      "             aria-label=\"Open this file in GitHub Desktop\"\n",
      "             data-ga-click=\"Repository, open with desktop\">\n",
      "              <svg class=\"octicon octicon-device-desktop\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M1.75 2.5h12.5a.25.25 0 01.25.25v7.5a.25.25 0 01-.25.25H1.75a.25.25 0 01-.25-.25v-7.5a.25.25 0 01.25-.25zM14.25 1H1.75A1.75 1.75 0 000 2.75v7.5C0 11.216.784 12 1.75 12h3.727c-.1 1.041-.52 1.872-1.292 2.757A.75.75 0 004.75 16h6.5a.75.75 0 00.565-1.243c-.772-.885-1.193-1.716-1.292-2.757h3.727A1.75 1.75 0 0016 10.25v-7.5A1.75 1.75 0 0014.25 1zM9.018 12H6.982a5.72 5.72 0 01-.765 2.5h3.566a5.72 5.72 0 01-.765-2.5z\"></path></svg>\n",
      "          </a>\n",
      "\n",
      "          <a href=\"/login?return_to=%2Fsplicemachine%2Fpysplice%2Fblob%2Fmaster%2FREADME.md\" class=\"btn-octicon disabled tooltipped tooltipped-nw\"\n",
      "            aria-label=\"You must be signed in to make or propose changes\">\n",
      "            <svg class=\"octicon octicon-pencil\" height=\"16\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z\"></path></svg>\n",
      "          </a>\n",
      "          <a href=\"/login?return_to=%2Fsplicemachine%2Fpysplice%2Fblob%2Fmaster%2FREADME.md\" class=\"btn-octicon btn-octicon-danger disabled tooltipped tooltipped-nw\"\n",
      "            aria-label=\"You must be signed in to make or propose changes\">\n",
      "            <svg class=\"octicon octicon-trashcan\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M6.5 1.75a.25.25 0 01.25-.25h2.5a.25.25 0 01.25.25V3h-3V1.75zm4.5 0V3h2.25a.75.75 0 010 1.5H2.75a.75.75 0 010-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75zM4.496 6.675a.75.75 0 10-1.492.15l.66 6.6A1.75 1.75 0 005.405 15h5.19c.9 0 1.652-.681 1.741-1.576l.66-6.6a.75.75 0 00-1.492-.149l-.66 6.6a.25.25 0 01-.249.225h-5.19a.25.25 0 01-.249-.225l-.66-6.6z\"></path></svg>\n",
      "          </a>\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "      \n",
      "  <div id=\"readme\" class=\"Box-body readme blob js-code-block-container p-5 p-xl-6 gist-border-0\">\n",
      "    <article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p><a href=\"https://pysplice.readthedocs.io/en/latest/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/41e085e139a16f2686854cbbc83bc8980bceb1cd1e7014fc4fb265e96dfde0b3/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f707973706c6963652f62616467652f3f7374796c653d666c6174\" alt=\"Docs\" data-canonical-src=\"https://readthedocs.org/projects/pysplice/badge/?style=flat\" style=\"max-width:100%;\"></a></p>\n",
      "<h1><a id=\"user-content-splice-machine-python-package\" class=\"anchor\" aria-hidden=\"true\" href=\"#splice-machine-python-package\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Splice Machine Python Package</h1>\n",
      "<p>This package contains all of the classes and functions you need to interact with Splice Machine's scale out, Hadoop on SQL RDBMS from Python. It also contains several machine learning utilities for use with Apache Spark.</p>\n",
      "<h2><a id=\"user-content-installation-instructions-with-pip\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation-instructions-with-pip\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Installation Instructions: with Pip</h2>\n",
      "<p><code>(sudo) pip install git+https://github.com/splicemachine/pysplice</code></p>\n",
      "<h2><a id=\"user-content-modules\" class=\"anchor\" aria-hidden=\"true\" href=\"#modules\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Modules</h2>\n",
      "<p>This package contains 4 main external modules. First, <code>splicemachine.spark.context</code>, which houses our Python wrapped Native Spark Datasource, as well as our External Native Spark Datasource, for use outside of the Kubernetes Cluster. Second, <code>splicemachine.mlflow_support</code> which houses our Python interface to MLManager. Lastly, <code>splicemachine.stats</code> which houses functions/classes which simplify machine learning (by providing functions like Decision Tree Visualizers, Model Evaluators etc.) and <code>splicemachine.notebook</code> which provides Jupyter Notebook specific functionality like an embedded MLFlow UI and Spark Jobs UI.</p>\n",
      "<ol>\n",
      "<li><code>splicemachine.spark.context</code>: Native Spark Datasource for interacting with Splice Machine from Spark</li>\n",
      "</ol>\n",
      "<pre><code>class PySpliceContext(builtins.object)\n",
      " |  PySpliceContext(sparkSession, JDBC_URL=None, _unit_testing=False)\n",
      " |  \n",
      " |  This class implements a SpliceMachineContext object (similar to the SparkContext object)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession, JDBC_URL=None, _unit_testing=False)\n",
      " |      :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster\n",
      " |      :param sparkSession: (sparkContext) A SparkSession object for talking to Spark\n",
      " |  \n",
      " |  analyzeSchema(self, schema_name)\n",
      " |      analyze the schema\n",
      " |      :param schema_name: schema name which stats info will be collected\n",
      " |      :return:\n",
      " |  \n",
      " |  analyzeTable(self, schema_table_name, estimateStatistics=False, samplePercent=10.0)\n",
      " |      collect stats info on a table\n",
      " |      :param schema_table_name: full table name in the format of \"schema.table\"\n",
      " |      :param estimateStatistics:will use estimate statistics if True\n",
      " |      :param samplePercent:  the percentage or rows to be sampled.\n",
      " |      :return:\n",
      " |  \n",
      " |  bulkImportHFile(self, dataframe, schema_table_name, options)\n",
      " |      Bulk Import HFile from a dataframe into a schema.table\n",
      " |      :param dataframe: Input data\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param options: Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required\n",
      " |  \n",
      " |  bulkImportHFileWithRdd(self, rdd, schema, schema_table_name, options)\n",
      " |      Bulk Import HFile from an rdd into a schema.table\n",
      " |      :param rdd: Input data\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param options: Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required\n",
      " |  \n",
      " |  createDataFrame(self, rdd, schema)\n",
      " |      Creates a dataframe from a given rdd and schema.\n",
      " |      \n",
      " |      :param rdd: Input data\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |  \n",
      " |  createTable(self, dataframe, schema_table_name, primary_keys=None, create_table_options=None, to_upper=False, drop_table=False)\n",
      " |      Creates a schema.table from a dataframe\n",
      " |      :param dataframe: The Spark DataFrame to base the table off\n",
      " |      :param schema_table_name: str The schema.table to create\n",
      " |      :param primary_keys: List[str] the primary keys. Default None\n",
      " |      :param create_table_options: str The additional table-level SQL options default None\n",
      " |      :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      " |                          If False, the table will be created with lower case columns. Default False\n",
      " |      :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists,\n",
      " |                         the function will throw an exception.\n",
      " |  \n",
      " |  createTableWithSchema(self, schema_table_name, schema, keys=None, create_table_options=None)\n",
      " |      Creates a schema.table from a schema\n",
      " |      :param schema_table_name: str The schema.table to create\n",
      " |      :param schema: (StructType) The schema that describes the columns of the table\n",
      " |      :param keys: List[str] The primary keys. Default None\n",
      " |      :param create_table_options: str The additional table-level SQL options. Default None\n",
      " |  \n",
      " |  delete(self, dataframe, schema_table_name)\n",
      " |      Delete records in a dataframe based on joining by primary keys from the data frame.\n",
      " |      Be careful with column naming and case sensitivity.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to delete\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  deleteWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Delete records using an rdd based on joining by primary keys from the rdd.\n",
      " |      Be careful with column naming and case sensitivity.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD containing the primary keys you would like to delete from the table\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  df(self, sql)\n",
      " |      Return a Spark Dataframe from the results of a Splice Machine SQL Query\n",
      " |      \n",
      " |      :param sql: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return: A Spark DataFrame containing the results\n",
      " |  \n",
      " |  dropTable(self, schema_and_or_table_name, table_name=None)\n",
      " |      Drop a specified table.\n",
      " |      \n",
      " |      Call it like:\n",
      " |          dropTable('schemaName.tableName')\n",
      " |      Or:\n",
      " |          dropTable('schemaName', 'tableName')\n",
      " |      \n",
      " |      :param schema_and_or_table_name: (string) Pass the schema name in this param when passing the table_name param,\n",
      " |        or pass schemaName.tableName in this param without passing the table_name param\n",
      " |      :param table_name: (optional) (string) Table Name, used when schema_and_or_table_name contains only the schema name\n",
      " |  \n",
      " |  execute(self, query_string)\n",
      " |      execute a query\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return:\n",
      " |  \n",
      " |  executeUpdate(self, query_string)\n",
      " |      execute a dml query:(update,delete,drop,etc)\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return:\n",
      " |  \n",
      " |  export(self, dataframe, location, compression=False, replicationCount=1, fileEncoding=None, fieldSeparator=None, quoteCharacter=None)\n",
      " |      Export a dataFrame in CSV\n",
      " |      :param dataframe:\n",
      " |      :param location: Destination directory\n",
      " |      :param compression: Whether to compress the output or not\n",
      " |      :param replicationCount:  Replication used for HDFS write\n",
      " |      :param fileEncoding: fileEncoding or null, defaults to UTF-8\n",
      " |      :param fieldSeparator: fieldSeparator or null, defaults to ','\n",
      " |      :param quoteCharacter: quoteCharacter or null, defaults to '\"'\n",
      " |      :return:\n",
      " |  \n",
      " |  exportBinary(self, dataframe, location, compression, e_format)\n",
      " |      Export a dataFrame in binary format\n",
      " |      :param dataframe:\n",
      " |      :param location: Destination directory\n",
      " |      :param compression: Whether to compress the output or not\n",
      " |      :param e_format: Binary format to be used, currently only 'parquet' is supported\n",
      " |      :return:\n",
      " |  \n",
      " |  getConnection(self)\n",
      " |      Return a connection to the database\n",
      " |  \n",
      " |  getSchema(self, schema_table_name)\n",
      " |      Return the schema via JDBC.\n",
      " |      \n",
      " |      :param schema_table_name: (DF) Table name\n",
      " |  \n",
      " |  insert(self, dataframe, schema_table_name, to_upper=False)\n",
      " |      Insert a dataframe into a table (schema.table).\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to insert\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the DF\n",
      " |      :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      " |                          If False, the table will be created with lower case columns. Default False\n",
      " |  \n",
      " |  insertRdd(self, rdd, schema, schema_table_name)\n",
      " |      Insert an rdd into a table (schema.table).\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to insert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the RDD\n",
      " |  \n",
      " |  insertRddWithStatus(self, rdd, schema, schema_table_name, statusDirectory, badRecordsAllowed)\n",
      " |      Insert an rdd into a table (schema.table) while tracking and limiting records that fail to insert.\n",
      " |      The status directory and number of badRecordsAllowed allow for duplicate primary keys to be\n",
      " |      written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n",
      " |      to the status directory.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to insert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the dataframe\n",
      " |      :param statusDirectory The status directory where bad records file will be created\n",
      " |      :param badRecordsAllowed The number of bad records are allowed. -1 for unlimited\n",
      " |  \n",
      " |  insertWithStatus(self, dataframe, schema_table_name, statusDirectory, badRecordsAllowed)\n",
      " |      Insert a dataframe into a table (schema.table) while tracking and limiting records that fail to insert.\n",
      " |      The status directory and number of badRecordsAllowed allow for duplicate primary keys to be\n",
      " |      written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n",
      " |      to the status directory.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to insert\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the dataframe\n",
      " |      :param statusDirectory The status directory where bad records file will be created\n",
      " |      :param badRecordsAllowed The number of bad records are allowed. -1 for unlimited\n",
      " |  \n",
      " |  internalDf(self, query_string)\n",
      " |      SQL to Dataframe translation.  (Lazy)\n",
      " |      Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return: pyspark dataframe contains the result of query_string\n",
      " |  \n",
      " |  internalRdd(self, schema_table_name, column_projection=None)\n",
      " |      Table with projections in Splice mapped to an RDD.\n",
      " |      :param schema_table_name: (string) Accessed table\n",
      " |      :param column_projection: (list of strings) Names of selected columns\n",
      " |      :return RDD[Row] with the result of the projection\n",
      " |  \n",
      " |  rdd(self, schema_table_name, column_projection=None)\n",
      " |      Table with projections in Splice mapped to an RDD.\n",
      " |      :param schema_table_name: (string) Accessed table\n",
      " |      :param column_projection: (list of strings) Names of selected columns\n",
      " |      :return RDD[Row] with the result of the projection\n",
      " |  \n",
      " |  replaceDataframeSchema(self, dataframe, schema_table_name)\n",
      " |      Returns a dataframe with all column names replaced with the proper string case from the DB table\n",
      " |      :param dataframe: A dataframe with column names to convert\n",
      " |      :param schema_table_name: The schema.table with the correct column cases to pull from the database\n",
      " |  \n",
      " |  splitAndInsert(self, dataframe, schema_table_name, sample_fraction)\n",
      " |      Sample the dataframe, split the table, and insert a dataFrame into a schema.table.\n",
      " |      This corresponds to an insert into from select statement\n",
      " |      :param dataframe: Input data\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param sample_fraction: (float) A value between 0 and 1 that specifies the percentage of data in the dataFrame\n",
      " |          that should be sampled to determine the splits.\n",
      " |          For example, specify 0.005 if you want 0.5% of the data sampled.\n",
      " |  \n",
      " |  tableExists(self, schema_and_or_table_name, table_name=None)\n",
      " |      Check whether or not a table exists\n",
      " |      \n",
      " |      Call it like:\n",
      " |          tableExists('schemaName.tableName')\n",
      " |      Or:\n",
      " |          tableExists('schemaName', 'tableName')\n",
      " |      \n",
      " |      :param schema_and_or_table_name: (string) Pass the schema name in this param when passing the table_name param,\n",
      " |        or pass schemaName.tableName in this param without passing the table_name param\n",
      " |      :param table_name: (optional) (string) Table Name, used when schema_and_or_table_name contains only the schema name\n",
      " |  \n",
      " |  toUpper(self, dataframe)\n",
      " |      Returns a dataframe with all of the columns in uppercase\n",
      " |      :param dataframe: The dataframe to convert to uppercase\n",
      " |  \n",
      " |  truncateTable(self, schema_table_name)\n",
      " |      truncate a table\n",
      " |      :param schema_table_name: the full table name in the format \"schema.table_name\" which will be truncated\n",
      " |      :return:\n",
      " |  \n",
      " |  update(self, dataframe, schema_table_name)\n",
      " |      Update data from a dataframe for a specified schema_table_name (schema.table).\n",
      " |      The keys are required for the update and any other columns provided will be updated\n",
      " |      in the rows.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to update\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  updateWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Update data from an rdd for a specified schema_table_name (schema.table).\n",
      " |      The keys are required for the update and any other columns provided will be updated\n",
      " |      in the rows.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to use for updating the table\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  upsert(self, dataframe, schema_table_name)\n",
      " |      Upsert the data from a dataframe into a table (schema.table).\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to upsert\n",
      " |      :param schema_table_name: (string) The table in which you would like to upsert the RDD\n",
      " |  \n",
      " |  upsertWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Upsert the data from an RDD into a table (schema.table).\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to upsert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to upsert the RDD\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "</code></pre>\n",
      "<p>1.1) <code>splicemachine.spark.context</code>: External Native Spark Datasource for interacting with Splice Machine from Spark. Usage is identical to above after instantiation</p>\n",
      "<pre><code>class ExtPySpliceContext(PySpliceContext)\n",
      "|  ExtPySpliceContext(sparkSession, JDBC_URL=None, kafkaServers='localhost:9092', kafkaPollTimeout=20000, _unit_testing=False)\n",
      "|  \n",
      "|  This class implements a SplicemachineContext object from com.splicemachine.spark2\n",
      "|  \n",
      "|  Method resolution order:\n",
      "|      ExtPySpliceContext\n",
      "|      PySpliceContext\n",
      "|      builtins.object\n",
      "</code></pre>\n",
      "<ol start=\"2\">\n",
      "<li><code>splicemachine.mlflow_support</code>: MLFlow wrapped MLManager interface from Python. The majority of documentation is identical to <a href=\"https://www.mlflow.org/docs/1.6.0/index.html\" rel=\"nofollow\">MLflow</a>. Additional functions and functionality are below</li>\n",
      "</ol>\n",
      "<pre><code>USAGE\n",
      "   from splicemachine.mlflow_support import *\n",
      "   mlflow.register_splice_context(splice) # The Native Spark Datasource (or External)\n",
      "   mlflow.start_run()\n",
      "   mlflow.log_param(\"my\", \"param\")\n",
      "   mlflow.log_metric(\"score\", 100)\n",
      "   mlflow.end_run()\n",
      "You can also use syntax like this:\n",
      "   with mlflow.start_run() as run:\n",
      "        ...\n",
      "which automatically terminates the run at the end of the block.\n",
      "\n",
      "For the lower level client-side API, see `mlflow.client` available functions.\n",
      "\n",
      "ADDITIONAL FUNCTIONS\n",
      "\n",
      "register_splice_context(splice_context)\n",
      "  Register a Splice Context for Spark/Database operations. This is required before most other functions become available.\n",
      "  (artifact storage, for example)\n",
      "  :param splice_context:  splice context to input\n",
      "\n",
      "timer(timer_name, param=True)\n",
      "  Context manager for logging\n",
      "  :param timer_name:\n",
      "  :param param: whether or not to log the timer as a param (default=True). If false, logs as metric.\n",
      "  :return:\n",
      "  \n",
      "  Usage:\n",
      "  mlflow.start_run()\n",
      "  with mlflow.timer('run_time', param=False): # Log timer as param or metric\n",
      "      mlflow.log_param(\"my\", \"param\")\n",
      "      mlflow.log_metric(\"score\", 100)\n",
      "      mlflow.end_run()\n",
      "  You can also use the syntax like this:\n",
      "  with mlflow.start_run():\n",
      "      with mlflow.timer('run_time', param=False): # Log timer as param or metric\n",
      "          mlflow.log_param(\"my\", \"param\")\n",
      "          mlflow.log_metric(\"score\", 100)\n",
      "\n",
      "get_run_ids_by_name(run_name, experiment_id=None)\n",
      "  Gets a run id from the run name. If there are multiple runs with the same name, all run IDs are returned\n",
      "  :param run_name: The name of the run\n",
      "  :param experiment_id: The experiment to search in. If None, all experiments are searched\n",
      "  :return: List of run ids\n",
      "  \n",
      "current_run_id()\n",
      "  Retrieve the current run id\n",
      "  :return: the current run id\n",
      "  \n",
      "current_exp_id()\n",
      "  Retrieve the current exp id\n",
      "  :return: the current experiment id\n",
      "  \n",
      "lp(key, value):\n",
      "  Add a shortcut for logging parameters in MLFlow.\n",
      "  Accessible from mlflow.lp\n",
      "  :param key: key for the parameter\n",
      "  :param value: value for the parameter\n",
      "  \n",
      "lm(key, value, step=None):\n",
      "  Add a shortcut for logging metrics in MLFlow.\n",
      "  Accessible from mlflow.lm\n",
      "  :param key: key for the parameter\n",
      "  :param value: value for the parameter\n",
      "  \n",
      "log_model(model, name='model'):\n",
      "  Log a fitted spark pipeline/model or H2O model\n",
      "  :param model: (PipelineModel or Model) is the fitted Spark Model/Pipeline or H2O model to store\n",
      "      with the current run\n",
      "  :param name: (str) the run relative name to store the model under\n",
      "\n",
      "log_pipeline_stages(pipeline):\n",
      "  \"\"\"\n",
      "  Log the pipeline stages as params for the run. Currently only Spark Pipelines are supported\n",
      "  :param pipeline: fitted/unitted (spark) pipeline    \n",
      "  \n",
      "log_feature_transformations(unfit_pipeline):\n",
      "  Log feature transformations for an unfit pipeline\n",
      "  Logs --&gt; feature movement through the pipeline.\n",
      "  Currently only Spark pipelines are supported.\n",
      "  :param unfit_pipeline: unfit pipeline to log\n",
      "  \n",
      "log_model_params(pipeline_or_model):\n",
      "  Log the parameters of a fitted model or a\n",
      "  model part of a fitted pipeline. Currently only Spark models/Pipelines are supported\n",
      "  :param pipeline_or_model: fitted pipeline/fitted model\n",
      "  \n",
      "download_artifact(name, local_path, run_id=None):\n",
      "  Download the artifact at the given\n",
      "  run id (active default) + name\n",
      "  to the local path\n",
      "  :param name: (str) artifact name to load\n",
      "    (with respect to the run)\n",
      "  :param local_path: (str) local path to download the\n",
      "    model to. This path MUST include the file extension\n",
      "  :param run_id: (str) the run id to download the artifact\n",
      "    from. Defaults to active run\n",
      "    \n",
      "get_model_name(run_id):\n",
      "  Gets the model name associated with a run or None\n",
      "  :param run_id:\n",
      "  :return: str or None\n",
      "  \n",
      "load_model(run_id=None, name=None):\n",
      "  Download a model from database\n",
      "  and load it into Spark\n",
      "  :param run_id: the id of the run to get a model from\n",
      "      (the run must have an associated model with it named spark_model)\n",
      "  :param name: the name of the model in the database\n",
      "  \n",
      "login_director(username, password):\n",
      "  Authenticate into the MLManager Director\n",
      "  :param username: database username\n",
      "  :param password: database password\n",
      "  \n",
      "deploy_aws(app_name, region='us-east-2', instance_type='ml.m5.xlarge',\n",
      "              run_id=None, instance_count=1, deployment_mode='replace'):\n",
      "  \"\"\"\n",
      "  Queue Job to deploy a run to sagemaker with the\n",
      "  given run id (found in MLFlow UI or through search API)\n",
      "  :param run_id: the id of the run to deploy. Will default to the current\n",
      "      run id.\n",
      "  :param app_name: the name of the app in sagemaker once deployed\n",
      "  :param region: the sagemaker region to deploy to (us-east-2,\n",
      "      us-west-1, us-west-2, eu-central-1 supported)\n",
      "  :param instance_type: the EC2 Sagemaker instance type to deploy on\n",
      "      (ml.m4.xlarge supported)\n",
      "  :param instance_count: the number of instances to load balance predictions\n",
      "      on\n",
      "  :param deployment_mode: the method to deploy; create=application will fail\n",
      "      if an app with the name specified already exists; replace=application\n",
      "      in sagemaker will be replaced with this one if app already exists;\n",
      "      add=add the specified model to a prexisting application (not recommended)\n",
      "  \n",
      "deploy_azure(endpoint_name, resource_group, workspace, run_id=None, region='East US',\n",
      "                cpu_cores=0.1, allocated_ram=0.5, model_name=None):\n",
      "  Deploy a given run to AzureML.\n",
      "  :param endpoint_name: (str) the name of the endpoint in AzureML when deployed to\n",
      "      Azure Container Services. Must be unique.\n",
      "  :param resource_group: (str) Azure Resource Group for model. Automatically created if\n",
      "      it doesn't exist.\n",
      "  :param workspace: (str) the AzureML workspace to deploy the model under.\n",
      "      Will be created if it doesn't exist\n",
      "  :param run_id: (str) if specified, will deploy a previous run (\n",
      "      must have an spark model logged). Otherwise, will default to the active run\n",
      "  :param region: (str) AzureML Region to deploy to: Can be East US, East US 2, Central US,\n",
      "      West US 2, North Europe, West Europe or Japan East\n",
      "  :param cpu_cores: (float) Number of CPU Cores to allocate to the instance.\n",
      "      Can be fractional. Default=0.1\n",
      "  :param allocated_ram: (float) amount of RAM, in GB, allocated to the container.\n",
      "      Default=0.5\n",
      "  :param model_name: (str) If specified, this will be the name of the model in AzureML.\n",
      "      Otherwise, the model name will be randomly generated.\n",
      "      \n",
      "deploy_db(db_schema_name, db_table_name, run_id, primary_key=None, df=None, create_model_table=False, model_cols=None, classes=None, sklearn_args={}, verbose=False, pred_threshold=None, replace=False) -&gt; None\n",
      "  Function to deploy a trained (currently Spark, Sklearn or H2O) model to the Database.\n",
      "  This creates 2 tables: One with the features of the model, and one with the prediction and metadata.\n",
      "  They are linked with a column called MOMENT_ID\n",
      "  \n",
      "  :param db_schema_name: (str) the schema name to deploy to. If None, the currently set schema will be used.\n",
      "  :param db_table_name: (str) the table name to deploy to. If none, the run_id will be used for the table name(s)\n",
      "  :param run_id: (str) The run_id to deploy the model on. The model associated with this run will be deployed\n",
      "  \n",
      "  \n",
      "  OPTIONAL PARAMETERS:\n",
      "  :param primary_key: (List[Tuple[str, str]]) List of column + SQL datatype to use for the primary/composite key.\n",
      "                      If you are deploying to a table that already exists, this primary/composite key must exist in the table\n",
      "                      If you are creating the table in this function, you MUST pass in a primary key\n",
      "  :param df: (Spark or Pandas DF) The dataframe used to train the model\n",
      "              NOTE: this dataframe should NOT be transformed by the model. The columns in this df are the ones\n",
      "              that will be used to create the table.\n",
      "  :param create_model_table: Whether or not to create the table from the dataframe. Default false. This\n",
      "                              Will ONLY be used if the table does not exist and a dataframe is passed in\n",
      "  :param predictor_cols: (List[str]) The columns from the table to use for the model. If None, all columns in the table\n",
      "                                      will be passed to the model. If specified, the columns will be passed to the model\n",
      "                                      IN THAT ORDER. The columns passed here must exist in the table.\n",
      "  :param classes: (List[str]) The classes (prediction labels) for the model being deployed.\n",
      "                  NOTE: If not supplied, the table will have default column names for each class\n",
      "  :param sklearn_args: (dict{str: str}) Prediction options for sklearn models\n",
      "                      Available key value options:\n",
      "                      'predict_call': 'predict', 'predict_proba', or 'transform'\n",
      "                                                                     - Determines the function call for the model\n",
      "                                                                     If blank, predict will be used\n",
      "                                                                     (or transform if model doesn't have predict)\n",
      "                      'predict_args': 'return_std' or 'return_cov' - For Bayesian and Gaussian models\n",
      "                                                                       Only one can be specified\n",
      "                      If the model does not have the option specified, it will be ignored.\n",
      "  :param verbose: (bool) Whether or not to print out the queries being created. Helpful for debugging\n",
      "  :param pred_threshold: (double) A prediction threshold for *Keras* binary classification models\n",
      "                          If the model type isn't Keras, this parameter will be ignored\n",
      "                          NOTE: If the model type is Keras, the output layer has 1 node, and pred_threshold is None,\n",
      "                                you will NOT receive a class prediction, only the output of the final layer (like model.predict()).\n",
      "                                If you want a class prediction\n",
      "                                for your binary classification problem, you MUST pass in a threshold.\n",
      "  :param replace: (bool) whether or not to replace a currently existing model. This param does not yet work\n",
      "  \n",
      "  \n",
      "  This function creates the following:\n",
      "  IF you are creating the table from the dataframe:\n",
      "      * The model table where run_id is the run_id passed in (or the current active run_id\n",
      "          This will have a column for each feature in the feature vector. It will also contain:\n",
      "          USER which is the current user who made the request\n",
      "          EVAL_TIME which is the CURRENT_TIMESTAMP\n",
      "          the PRIMARY KEY column same as the DATA table to link predictions to rows in the table (primary key)\n",
      "          PREDICTION. The prediction of the model. If the :classes: param is not filled in, this will be default values for classification models\n",
      "          A column for each class of the predictor with the value being the probability/confidence of the model if applicable\n",
      "  IF you are deploying to an existing table:\n",
      "      * The table will be altered to include\n",
      "  \n",
      "  * A trigger that runs on (after) insertion to the data table that runs an INSERT into the prediction table,\n",
      "      calling the PREDICT function, passing in the row of data as well as the schema of the dataset, and the run_id of the model to run\n",
      "  * A trigger that runs on (after) insertion to the prediction table that calls an UPDATE to the row inserted,\n",
      "      parsing the prediction probabilities and filling in proper column values\n",
      "      \n",
      "get_deployed_models() -&gt; PandasDF:\n",
      "  \"\"\"\n",
      "  Get the currently deployed models in the database\n",
      "  :return: Pandas df\n",
      "</code></pre>\n",
      "<p>3.1) <code>splicemachine.stats</code>: houses utilities for machine learning</p>\n",
      "<pre><code>    class DecisionTreeVisualizer(builtins.object)\n",
      "     |  Visualize a decision tree, either in code like format, or graphviz\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  add_node(dot, parent, node_hash, root, realroot=False)\n",
      "     |      Traverse through the .debugString json and generate a graphviz tree\n",
      "     |      :param dot: dot file object\n",
      "     |      :param parent: not used currently\n",
      "     |      :param node_hash: unique node id\n",
      "     |      :param root: the root of tree\n",
      "     |      :param realroot: whether or not it is the real root, or a recursive root\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  feature_importance(spark, model, dataset, featuresCol='features')\n",
      "     |      Return a dataframe containing the relative importance of each feature\n",
      "     |      :param model:\n",
      "     |      :param dataframe:\n",
      "     |      :param featureCol:\n",
      "     |      :return: dataframe containing importance\n",
      "     |  \n",
      "     |  parse(lines)\n",
      "     |      Lines in debug string\n",
      "     |      :param lines:\n",
      "     |      :return: block json\n",
      "     |  \n",
      "     |  replacer(string, bad, good)\n",
      "     |      Replace every string in \"bad\" with the corresponding string in \"good\"\n",
      "     |      :param string: string to replace in\n",
      "     |      :param bad: array of strings to replace\n",
      "     |      :param good: array of strings to replace with\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  tree_json(tree)\n",
      "     |      Generate a JSON representation of a decision tree\n",
      "     |      :param tree: tree debug string\n",
      "     |      :return: json\n",
      "     |  \n",
      "     |  visualize(model, feature_column_names, label_names, size=None, horizontal=False, tree_name='tree', visual=False)\n",
      "     |      Visualize a decision tree, either in a code like format, or graphviz\n",
      "     |      :param model: the fitted decision tree classifier\n",
      "     |      :param feature_column_names: (List[str]) column names for features\n",
      "     |             You can access these feature names by using your VectorAssembler (in PySpark) and calling it's .getInputCols() function\n",
      "     |      :param label_names: (List[str]) labels vector (below avg, above avg)\n",
      "     |      :param size: tuple(int,int) The size of the graph. If unspecified, graphviz will automatically assign a size\n",
      "     |      :param horizontal: (Bool) if the tree should be rendered horizontally\n",
      "     |      :param tree_name: the name you would like to call the tree\n",
      "     |      :param visual: bool, true if you want a graphviz pdf containing your file\n",
      "     |      :return dot: The graphvis object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class IndReconstructer(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  IndReconstructer(inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  Transformer to reconstruct String Index from OneHotDummy Columns. This can be used as a part of a Pipeline Ojbect\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: Transformed PySpark Dataframe With Original String Indexed Variables\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndReconstructer\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputCol=None, outputCol=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  setParams(self, inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class MarkovChain(builtins.object)\n",
      "     |  MarkovChain(transition_prob)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, transition_prob)\n",
      "     |      Initialize the MarkovChain instance.\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transition_prob: dict\n",
      "     |          A dict object representing the transition\n",
      "     |          probabilities in Markov Chain.\n",
      "     |          Should be of the form:\n",
      "     |              {'state1': {'state1': 0.1, 'state2': 0.4},\n",
      "     |               'state2': {...}}\n",
      "     |  \n",
      "     |  generate_states(self, current_state, no=10, last=True)\n",
      "     |      Generates the next states of the system.\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      current_state: str\n",
      "     |          The state of the current random variable.\n",
      "     |      no: int\n",
      "     |          The number of future states to generate.\n",
      "     |      last: bool\n",
      "     |          Do we want to return just the last value\n",
      "     |  \n",
      "     |  get_max_num_steps(self)\n",
      "     |  \n",
      "     |  next_state(self, current_state)\n",
      "     |      Returns the state of the random variable at the next time\n",
      "     |      instance.\n",
      "     |      :param current_state: The current state of the system.\n",
      "     |      :raises: Exception if random choice fails\n",
      "     |      :return: next state\n",
      "     |  \n",
      "     |  rep_states(self, current_state, no=10, num_reps=10)\n",
      "     |      running generate states a bunch of times and returning the final state that happens the most\n",
      "     |      Arguments:\n",
      "     |          current_state str -- The state of the current random variable\n",
      "     |          no int -- number of time steps in the future to run\n",
      "     |          num_reps int -- number of times to run the simultion forward\n",
      "     |      Returns:\n",
      "     |          state -- the most commonly reached state at the end of these runs\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OneHotDummies(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  OneHotDummies(inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  Transformer to generate dummy columns for categorical variables as a part of a preprocessing pipeline\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Classes\n",
      "     |  :param HasInputCol: Inherited Classes\n",
      "     |  :param HasOutputCol: Inherited Classes\n",
      "     |  :return: pyspark DataFrame\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotDummies\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputCol=None, outputCol=None)\n",
      "     |      Assigns variables to parameters passed\n",
      "     |      :param inputCol: Sparse vector returned by OneHotEncoders, defaults to None\n",
      "     |      :param outputCol: string base to append to output columns names, defaults to None\n",
      "     |  \n",
      "     |  getOutCols(self)\n",
      "     |  \n",
      "     |  setParams(self, inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OverSampleCrossValidator(pyspark.ml.tuning.CrossValidator)\n",
      "     |  OverSampleCrossValidator(estimator, estimatorParamMaps, evaluator, numFolds=3, seed=None, parallelism=3, collectSubModels=False, labelCol='label', altEvaluators=None, overSample=True)\n",
      "     |  \n",
      "     |  Class to perform Cross Validation model evaluation while over-sampling minority labels.\n",
      "     |  Example:\n",
      "     |  -------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.classification import LogisticRegression\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.linalg import Vectors\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import OverSampleCrossValidator\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; dataset = spark.createDataFrame(\n",
      "     |  ...      [(Vectors.dense([0.0]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.5]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.4]), 1.0),\n",
      "     |  ...       (Vectors.dense([0.6]), 1.0),\n",
      "     |  ...       (Vectors.dense([1.0]), 1.0)] * 10,\n",
      "     |  ...      [\"features\", \"label\"])\n",
      "     |  &gt;&gt;&gt; lr = LogisticRegression()\n",
      "     |  &gt;&gt;&gt; grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
      "     |  &gt;&gt;&gt; PRevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderPR')\n",
      "     |  &gt;&gt;&gt; AUCevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
      "     |  &gt;&gt;&gt; ACCevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
      "     |  &gt;&gt;&gt; cv = OverSampleCrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=AUCevaluator, altEvaluators = [PRevaluator, ACCevaluator],parallelism=2,seed = 1234)\n",
      "     |  &gt;&gt;&gt; cvModel = cv.fit(dataset)\n",
      "     |  &gt;&gt;&gt; print(cvModel.avgMetrics)\n",
      "     |  [(0.5, [0.5888888888888888, 0.3888888888888889]), (0.806878306878307, [0.8556863149300125, 0.7055555555555556])]\n",
      "     |  &gt;&gt;&gt; print(AUCevaluator.evaluate(cvModel.transform(dataset)))\n",
      "     |  0.8333333333333333\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OverSampleCrossValidator\n",
      "     |      pyspark.ml.tuning.CrossValidator\n",
      "     |      pyspark.ml.base.Estimator\n",
      "     |      pyspark.ml.tuning._CrossValidatorParams\n",
      "     |      pyspark.ml.tuning._ValidatorParams\n",
      "     |      pyspark.ml.param.shared.HasSeed\n",
      "     |      pyspark.ml.param.shared.HasParallelism\n",
      "     |      pyspark.ml.param.shared.HasCollectSubModels\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, estimatorParamMaps, evaluator, numFolds=3, seed=None, parallelism=3, collectSubModels=False, labelCol='label', altEvaluators=None, overSample=True)\n",
      "     |      Initialize Self\n",
      "     |      :param estimator: Machine Learning Model, defaults to None\n",
      "     |      :param estimatorParamMaps: paramMap to search, defaults to None\n",
      "     |      :param evaluator: primary model evaluation metric, defaults to None\n",
      "     |      :param numFolds: number of folds to perform, defaults to 3\n",
      "     |      :param seed: random state, defaults to None\n",
      "     |      :param parallelism: number of threads, defaults to 1\n",
      "     |      :param collectSubModels: to return submodels, defaults to False\n",
      "     |      :param labelCol: target variable column label, defaults to 'label'\n",
      "     |      :param altEvaluators: additional metrics to evaluate, defaults to None\n",
      "     |                           If passed, the metrics of the alternate evaluators are accessed in the CrossValidatorModel.avgMetrics attribute\n",
      "     |      :param overSample: Boolean: to perform oversampling of minority labels, defaults to True\n",
      "     |  \n",
      "     |  getAltEvaluators(self)\n",
      "     |  \n",
      "     |  getLabel(self)\n",
      "     |  \n",
      "     |  getOversample(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.tuning.CrossValidator:\n",
      "     |  \n",
      "     |  copy(self, extra=None)\n",
      "     |      Creates a copy of this instance with a randomly generated uid\n",
      "     |      and some extra params. This copies creates a deep copy of\n",
      "     |      the embedded paramMap, and copies the embedded and extra parameters over.\n",
      "     |      \n",
      "     |      :param extra: Extra parameters to copy to the new instance\n",
      "     |      :return: Copy of this instance\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setCollectSubModels(self, value)\n",
      "     |      Sets the value of :py:attr:`collectSubModels`.\n",
      "     |  \n",
      "     |  setEstimator(self, value)\n",
      "     |      Sets the value of :py:attr:`estimator`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setEstimatorParamMaps(self, value)\n",
      "     |      Sets the value of :py:attr:`estimatorParamMaps`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setEvaluator(self, value)\n",
      "     |      Sets the value of :py:attr:`evaluator`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setNumFolds(self, value)\n",
      "     |      Sets the value of :py:attr:`numFolds`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setParallelism(self, value)\n",
      "     |      Sets the value of :py:attr:`parallelism`.\n",
      "     |  \n",
      "     |  setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3, seed=None, parallelism=1, collectSubModels=False)\n",
      "     |      setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,                  seed=None, parallelism=1, collectSubModels=False):\n",
      "     |      Sets params for cross validator.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setSeed(self, value)\n",
      "     |      Sets the value of :py:attr:`seed`.\n",
      "     |  \n",
      "     |  write(self)\n",
      "     |      Returns an MLWriter instance for this ML instance.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pyspark.ml.tuning.CrossValidator:\n",
      "     |  \n",
      "     |  read() from builtins.type\n",
      "     |      Returns an MLReader instance for this class.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OverSampler(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  OverSampler(labelCol=None, strategy='auto', randomState=None)\n",
      "     |  \n",
      "     |  Transformer to oversample datapoints with minority labels\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: PySpark Dataframe with labels in approximately equal ratios\n",
      "     |  Example:\n",
      "     |  -------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql import functions as F\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.linalg import Vectors\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import OverSampler\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; df = spark.createDataFrame(\n",
      "     |  ...      [(Vectors.dense([0.0]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.5]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.4]), 1.0),\n",
      "     |  ...       (Vectors.dense([0.6]), 1.0),\n",
      "     |  ...       (Vectors.dense([1.0]), 1.0)] * 10,\n",
      "     |  ...      [\"features\", \"Class\"])\n",
      "     |  &gt;&gt;&gt; df.groupBy(F.col(\"Class\")).count().orderBy(\"count\").show()\n",
      "     |  +-----+-----+\n",
      "     |  |Class|count|\n",
      "     |  +-----+-----+\n",
      "     |  |  0.0|   20|\n",
      "     |  |  1.0|   30|\n",
      "     |  +-----+-----+\n",
      "     |  &gt;&gt;&gt; oversampler = OverSampler(labelCol = \"Class\", strategy = \"auto\")\n",
      "     |  &gt;&gt;&gt; oversampler.transform(df).groupBy(\"Class\").count().show()\n",
      "     |  +-----+-----+\n",
      "     |  |Class|count|\n",
      "     |  +-----+-----+\n",
      "     |  |  0.0|   29|\n",
      "     |  |  1.0|   30|\n",
      "     |  +-----+-----+\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OverSampler\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labelCol=None, strategy='auto', randomState=None)\n",
      "     |      Initialize self\n",
      "     |      :param labelCol: Label Column name, defaults to None\n",
      "     |      :param strategy: defaults to \"auto\", strategy to resample the dataset:\n",
      "     |                      • Only currently supported for \"auto\" Corresponds to random samples with repleaement\n",
      "     |      :param randomState: sets the seed of sample algorithm\n",
      "     |  \n",
      "     |  setParams(self, labelCol=None, strategy='auto')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  deserializeFromBundle(path)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class Rounder(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  Rounder(predictionCol='prediction', labelCol='label', clipPreds=True, maxLabel=None, minLabel=None)\n",
      "     |  \n",
      "     |  Transformer to round predictions for ordinal regression\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: Transformed Dataframe with rounded predictionCol\n",
      "     |  Example:\n",
      "     |  --------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import Rounder\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; dataset = spark.createDataFrame(\n",
      "     |  ...      [(0.2, 0.0),\n",
      "     |  ...       (1.2, 1.0),\n",
      "     |  ...       (1.6, 2.0),\n",
      "     |  ...       (1.1, 0.0),\n",
      "     |  ...       (3.1, 0.0)],\n",
      "     |  ...      [\"prediction\", \"label\"])\n",
      "     |  &gt;&gt;&gt; dataset.show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.2|  0.0|\n",
      "     |  |       1.2|  1.0|\n",
      "     |  |       1.6|  2.0|\n",
      "     |  |       1.1|  0.0|\n",
      "     |  |       3.1|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  &gt;&gt;&gt; rounder = Rounder(predictionCol = \"prediction\", labelCol = \"label\", clipPreds = True)\n",
      "     |  &gt;&gt;&gt; rounder.transform(dataset).show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.0|  0.0|\n",
      "     |  |       1.0|  1.0|\n",
      "     |  |       2.0|  2.0|\n",
      "     |  |       1.0|  0.0|\n",
      "     |  |       2.0|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  &gt;&gt;&gt; rounderNoClip = Rounder(predictionCol = \"prediction\", labelCol = \"label\", clipPreds = False)\n",
      "     |  &gt;&gt;&gt; rounderNoClip.transform(dataset).show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.0|  0.0|\n",
      "     |  |       1.0|  1.0|\n",
      "     |  |       2.0|  2.0|\n",
      "     |  |       1.0|  0.0|\n",
      "     |  |       3.0|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Rounder\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, predictionCol='prediction', labelCol='label', clipPreds=True, maxLabel=None, minLabel=None)\n",
      "     |      initialize self\n",
      "     |      :param predictionCol: column containing predictions, defaults to \"prediction\"\n",
      "     |      :param labelCol: column containing labels, defaults to \"label\"\n",
      "     |      :param clipPreds: clip all predictions above a specified maximum value\n",
      "     |      :param maxLabel: optional: the maximum value for the prediction column, otherwise uses the maximum of the labelCol, defaults to None\n",
      "     |      :param minLabel: optional: the minimum value for the prediction column, otherwise uses the maximum of the labelCol, defaults to None\n",
      "     |  \n",
      "     |  setParams(self, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  deserializeFromBundle(path)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "     class SpliceBaseEvaluator(builtins.object)\n",
      "     |  SpliceBaseEvaluator(spark, evaluator, supported_metrics, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, evaluator, supported_metrics, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class SpliceBinaryClassificationEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceBinaryClassificationEvaluator(spark, predictionCol='prediction', labelCol='label', confusion_matrix=True)\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceBinaryClassificationEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label', confusion_matrix=True)\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Evaluate actual vs Predicted in a dataframe\n",
      "     |      :param predictions_dataframe: the dataframe containing the label and the predicition\n",
      "     |  \n",
      "     |  plotROC(self, fittedEstimator, ax)\n",
      "     |      Plots the receiver operating characteristic curve for the trained classifier\n",
      "     |      :param fittedEstimator: fitted logistic regression model\n",
      "     |      :param ax: matplotlib axis object\n",
      "     |      :return: axis with ROC plot\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class SpliceMultiClassificationEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceMultiClassificationEvaluator(spark, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceMultiClassificationEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SpliceRegressionEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceRegressionEvaluator(spark, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Splice Regression Evaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceRegressionEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    best_fit_distribution(data, col_name, bins, ax)\n",
      "        Model data by finding best fit distribution to data\n",
      "        :param data: DataFrame with one column containing the feature whose distribution is to be investigated\n",
      "        :param col_name: column name for feature\n",
      "        :param bins: number of bins to use in generating the histogram of this data\n",
      "        :param ax: axis to plot histogram on\n",
      "        :return: (best_distribution.name, best_params, best_sse)\n",
      "            best_distribution.name: string of the best distribution name\n",
      "            best_params: parameters for this distribution\n",
      "            best_sse: sum of squared errors for this distribution against the empirical pdf\n",
      "    \n",
      "    estimateCovariance(df, features_col='features')\n",
      "        Compute the covariance matrix for a given dataframe.\n",
      "            Note: The multi-dimensional covariance array should be calculated using outer products.  Don't forget to normalize the data by first subtracting the mean.\n",
      "        :param df: PySpark dataframe\n",
      "        :param features_col: name of the column with the features, defaults to 'features'\n",
      "        :return: np.ndarray: A multi-dimensional array where the number of rows and columns both equal the length of the arrays in the input dataframe.\n",
      "    \n",
      "    get_confusion_matrix(spark, TP, TN, FP, FN)\n",
      "        function that shows you a device called a confusion matrix... will be helpful when evaluating.\n",
      "        It allows you to see how well your model performs\n",
      "        :param TP: True Positives\n",
      "        :param TN: True Negatives\n",
      "        :param FP: False Positives\n",
      "        :param FN: False Negatives\n",
      "    \n",
      "    get_string_pipeline(df, cols_to_exclude, steps=['StringIndexer', 'OneHotEncoder', 'OneHotDummies'])\n",
      "        Generates a list of preprocessing stages\n",
      "        :param df: DataFrame including only the training data\n",
      "        :param cols_to_exclude: Column names we don't want to to include in the preprocessing (i.e. SUBJECT/ target column)\n",
      "        :param stages: preprocessing steps to take\n",
      "        :return:  (stages, Numeric_Columns)\n",
      "            stages: list of pipeline stages to be used in preprocessing\n",
      "            Numeric_Columns: list of columns that contain numeric features\n",
      "    \n",
      "    inspectTable(spliceMLCtx, sql, topN=5)\n",
      "        Inspect the values of the columns of the table (dataframe) returned from the sql query\n",
      "        :param spliceMLCtx: SpliceMLContext\n",
      "        :param sql: sql string to execute\n",
      "        :param topN: the number of most frequent elements of a column to return, defaults to 5\n",
      "    \n",
      "    make_pdf(dist, params, size=10000)\n",
      "        Generate distributions's Probability Distribution Function\n",
      "        :param dist: scipy.stats distribution object: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
      "        :param params: distribution parameters\n",
      "        :param size: how many data points to generate , defaults to 10000\n",
      "        :return: series of probability density function for this distribution\n",
      "    \n",
      "    pca_with_scores(df, k=10)\n",
      "        Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
      "        Note:\n",
      "            All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
      "            each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
      "        :param df:  A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
      "        :param k: The number of principal components to return., defaults to 10\n",
      "        :return:(eigenvectors, `RDD` of scores, eigenvalues).\n",
      "            Eigenvectors: multi-dimensional array where the number of\n",
      "            rows equals the length of the arrays in the input `RDD` and the number of columns equals`k`.\n",
      "            `RDD` of scores: has the same number of rows as `data` and consists of arrays of length `k`.\n",
      "            Eigenvalues is an array of length d (the number of features).\n",
      "    \n",
      "    postprocessing_pipeline(df, cols_to_exclude)\n",
      "        Assemble postprocessing pipeline to reconstruct original categorical indexed values from OneHotDummy Columns\n",
      "        :param df: DataFrame Including the original string Columns\n",
      "        :param cols_to_exclude: list of columns to exclude\n",
      "        :return: (reconstructers, String_Columns)\n",
      "            reconstructers: list of IndReconstructer stages\n",
      "            String_Columns: list of columns that are being reconstructed\n",
      "    \n",
      "    reconstructPCA(sql, df, pc, mean, std, originalColumns, fits, pcaColumn='pcaFeatures')\n",
      "        Reconstruct data from lower dimensional space after performing PCA\n",
      "        :param sql: SQLContext\n",
      "        :param df: PySpark DataFrame: inputted PySpark DataFrame\n",
      "        :param pc: numpy.ndarray: principal components projected onto\n",
      "        :param mean: numpy.ndarray: mean of original columns\n",
      "        :param std: numpy.ndarray: standard deviation of original columns\n",
      "        :param originalColumns: list: original column names\n",
      "        :param fits: fits of features returned from best_fit_distribution\n",
      "        :param pcaColumn: column in df that contains PCA features, defaults to 'pcaFeatures'\n",
      "        :return: dataframe containing reconstructed data\n",
      "    \n",
      "    varianceExplained(df, k=10)\n",
      "        returns the proportion of variance explained by `k` principal componenets. Calls the above PCA procedure\n",
      "        :param df: PySpark DataFrame\n",
      "        :param k: number of principal components , defaults to 10\n",
      "        :return: (proportion, principal_components, scores, eigenvalues)\n",
      "    \n",
      "    vector_assembler_pipeline(df, columns, doPCA=False, k=10)\n",
      "        After preprocessing String Columns, this function can be used to assemble a feature vector to be used for learning\n",
      "        creates the following stages: VectorAssembler -&gt; Standard Scalar [{ -&gt; PCA}]\n",
      "        :param df: DataFrame containing preprocessed Columns\n",
      "        :param columns: list of Column names of the preprocessed columns\n",
      "        :param doPCA:  Do you want to do PCA as part of the vector assembler? defaults to False\n",
      "        :param k:  Number of Principal Components to use, defaults to 10\n",
      "        :return: List of vector assembling stages\n",
      "</code></pre>\n",
      "<p>3.1) <code>splicemachine.stats</code>: houses utilities for use in Jupyter Notebooks running in the Kubernetes cloud environment\n",
      "FUNCTIONS\n",
      "get_mlflow_ui()</p>\n",
      "<pre><code>get_spark_ui(port=None, spark_session=None)\n",
      "\n",
      "hide_toggle(toggle_next=False)\n",
      "    Function to add a toggle at the bottom of Jupyter Notebook cells to allow the entire cell to be collapsed.\n",
      "    :param toggle_next: Bool determine if the toggle should affect the current cell or the next cell\n",
      "    Usage: from splicemachine.stats.utilities import hide_toggle\n",
      "           hide_toggle()\n",
      "</code></pre>\n",
      "<pre><code></code></pre>\n",
      "</article>\n",
      "  </div>\n",
      "\n",
      "    </div>\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  <details class=\"details-reset details-overlay details-overlay-dark\" id=\"jumpto-line-details-dialog\">\n",
      "    <summary data-hotkey=\"l\" aria-label=\"Jump to line\"></summary>\n",
      "    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast linejump\" aria-label=\"Jump to line\">\n",
      "      <!-- '\"` --><!-- </textarea></xmp> --></option></form><form class=\"js-jump-to-line-form Box-body d-flex\" action=\"\" accept-charset=\"UTF-8\" method=\"get\">\n",
      "        <input class=\"form-control flex-auto mr-3 linejump-input js-jump-to-line-field\" type=\"text\" placeholder=\"Jump to line&hellip;\" aria-label=\"Jump to line\" autofocus>\n",
      "        <button type=\"submit\" class=\"btn\" data-close-dialog>Go</button>\n",
      "</form>    </details-dialog>\n",
      "  </details>\n",
      "\n",
      "    <div class=\"Popover anim-scale-in js-tagsearch-popover\"\n",
      "     hidden\n",
      "     data-tagsearch-url=\"/splicemachine/pysplice/find-definition\"\n",
      "     data-tagsearch-ref=\"master\"\n",
      "     data-tagsearch-path=\"README.md\"\n",
      "     data-tagsearch-lang=\"Markdown\"\n",
      "     data-hydro-click=\"{&quot;event_type&quot;:&quot;code_navigation.click_on_symbol&quot;,&quot;payload&quot;:{&quot;action&quot;:&quot;click_on_symbol&quot;,&quot;repository_id&quot;:142071553,&quot;ref&quot;:&quot;master&quot;,&quot;language&quot;:&quot;Markdown&quot;,&quot;originating_url&quot;:&quot;https://github.com/splicemachine/pysplice/blob/master/README.md&quot;,&quot;user_id&quot;:null}}\"\n",
      "     data-hydro-click-hmac=\"e6c52d61491135e129d1c8d88def837112c94f05dddb402d92bd097cd7c0f3fe\">\n",
      "  <div class=\"Popover-message Popover-message--large Popover-message--top-left TagsearchPopover mt-1 mb-4 mx-auto Box box-shadow-large\">\n",
      "    <div class=\"TagsearchPopover-content js-tagsearch-popover-content overflow-auto\" style=\"will-change:transform;\">\n",
      "    </div>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "    </main>\n",
      "  </div>\n",
      "\n",
      "  </div>\n",
      "\n",
      "          \n",
      "<div class=\"footer container-xl width-full p-responsive\" role=\"contentinfo\">\n",
      "  <div class=\"position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light \">\n",
      "    <ul class=\"list-style-none d-flex flex-wrap col-12 col-lg-5 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0\">\n",
      "      <li class=\"mr-3 mr-lg-0\">&copy; 2020 GitHub, Inc.</li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a data-ga-click=\"Footer, go to terms, text:terms\" href=\"https://github.com/site/terms\">Terms</a></li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a data-ga-click=\"Footer, go to privacy, text:privacy\" href=\"https://github.com/site/privacy\">Privacy</a></li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a data-ga-click=\"Footer, go to security, text:security\" href=\"https://github.com/security\">Security</a></li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a href=\"https://githubstatus.com/\" data-ga-click=\"Footer, go to status, text:status\">Status</a></li>\n",
      "        <li><a data-ga-click=\"Footer, go to help, text:help\" href=\"https://docs.github.com\">Help</a></li>\n",
      "    </ul>\n",
      "\n",
      "    <a aria-label=\"Homepage\" title=\"GitHub\" class=\"footer-octicon d-none d-lg-block mx-lg-4\" href=\"https://github.com\">\n",
      "      <svg height=\"24\" class=\"octicon octicon-mark-github\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"24\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\"></path></svg>\n",
      "</a>\n",
      "    <ul class=\"list-style-none d-flex flex-wrap col-12 col-lg-5 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0\">\n",
      "        <li class=\"mr-3 mr-lg-0\"><a data-ga-click=\"Footer, go to contact, text:contact\" href=\"https://github.com/contact\">Contact GitHub</a></li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a href=\"https://github.com/pricing\" data-ga-click=\"Footer, go to Pricing, text:Pricing\">Pricing</a></li>\n",
      "      <li class=\"mr-3 mr-lg-0\"><a href=\"https://docs.github.com\" data-ga-click=\"Footer, go to api, text:api\">API</a></li>\n",
      "      <li class=\"mr-3 mr-lg-0\"><a href=\"https://services.github.com\" data-ga-click=\"Footer, go to training, text:training\">Training</a></li>\n",
      "        <li class=\"mr-3 mr-lg-0\"><a href=\"https://github.blog\" data-ga-click=\"Footer, go to blog, text:blog\">Blog</a></li>\n",
      "        <li><a data-ga-click=\"Footer, go to about, text:about\" href=\"https://github.com/about\">About</a></li>\n",
      "    </ul>\n",
      "  </div>\n",
      "  <div class=\"d-flex flex-justify-center pb-6\">\n",
      "    <span class=\"f6 text-gray-light\"></span>\n",
      "  </div>\n",
      "\n",
      "  \n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "  <div id=\"ajax-error-message\" class=\"ajax-error-message flash flash-error\">\n",
      "    <svg class=\"octicon octicon-alert\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z\"></path></svg>\n",
      "    <button type=\"button\" class=\"flash-close js-ajax-error-dismiss\" aria-label=\"Dismiss error\">\n",
      "      <svg class=\"octicon octicon-x\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M3.72 3.72a.75.75 0 011.06 0L8 6.94l3.22-3.22a.75.75 0 111.06 1.06L9.06 8l3.22 3.22a.75.75 0 11-1.06 1.06L8 9.06l-3.22 3.22a.75.75 0 01-1.06-1.06L6.94 8 3.72 4.78a.75.75 0 010-1.06z\"></path></svg>\n",
      "    </button>\n",
      "    You can’t perform that action at this time.\n",
      "  </div>\n",
      "\n",
      "\n",
      "  <div class=\"js-stale-session-flash flash flash-warn flash-banner\" hidden\n",
      "    >\n",
      "    <svg class=\"octicon octicon-alert\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z\"></path></svg>\n",
      "    <span class=\"js-stale-session-flash-signed-in\" hidden>You signed in with another tab or window. <a href=\"\">Reload</a> to refresh your session.</span>\n",
      "    <span class=\"js-stale-session-flash-signed-out\" hidden>You signed out in another tab or window. <a href=\"\">Reload</a> to refresh your session.</span>\n",
      "  </div>\n",
      "    <template id=\"site-details-dialog\">\n",
      "  <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\" open>\n",
      "    <summary role=\"button\" aria-label=\"Close dialog\"></summary>\n",
      "    <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\">\n",
      "      <button class=\"Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0\" type=\"button\" aria-label=\"Close dialog\" data-close-dialog>\n",
      "        <svg class=\"octicon octicon-x\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M3.72 3.72a.75.75 0 011.06 0L8 6.94l3.22-3.22a.75.75 0 111.06 1.06L9.06 8l3.22 3.22a.75.75 0 11-1.06 1.06L8 9.06l-3.22 3.22a.75.75 0 01-1.06-1.06L6.94 8 3.72 4.78a.75.75 0 010-1.06z\"></path></svg>\n",
      "      </button>\n",
      "      <div class=\"octocat-spinner my-6 js-details-dialog-spinner\"></div>\n",
      "    </details-dialog>\n",
      "  </details>\n",
      "</template>\n",
      "\n",
      "    <div class=\"Popover js-hovercard-content position-absolute\" style=\"display: none; outline: none;\" tabindex=\"0\">\n",
      "  <div class=\"Popover-message Popover-message--bottom-left Popover-message--large Box box-shadow-large\" style=\"width:360px;\">\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "\n",
      "  </body>\n",
      "</html>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('https://github.com/splicemachine/pysplice/blob/master/README.md')\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<code>(sudo) pip install git+https://github.com/splicemachine/pysplice</code>, <code>splicemachine.spark.context</code>, <code>splicemachine.mlflow_support</code>, <code>splicemachine.stats</code>, <code>splicemachine.notebook</code>, <code>splicemachine.spark.context</code>, <code>class PySpliceContext(builtins.object)\n",
      " |  PySpliceContext(sparkSession, JDBC_URL=None, _unit_testing=False)\n",
      " |  \n",
      " |  This class implements a SpliceMachineContext object (similar to the SparkContext object)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkSession, JDBC_URL=None, _unit_testing=False)\n",
      " |      :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster\n",
      " |      :param sparkSession: (sparkContext) A SparkSession object for talking to Spark\n",
      " |  \n",
      " |  analyzeSchema(self, schema_name)\n",
      " |      analyze the schema\n",
      " |      :param schema_name: schema name which stats info will be collected\n",
      " |      :return:\n",
      " |  \n",
      " |  analyzeTable(self, schema_table_name, estimateStatistics=False, samplePercent=10.0)\n",
      " |      collect stats info on a table\n",
      " |      :param schema_table_name: full table name in the format of \"schema.table\"\n",
      " |      :param estimateStatistics:will use estimate statistics if True\n",
      " |      :param samplePercent:  the percentage or rows to be sampled.\n",
      " |      :return:\n",
      " |  \n",
      " |  bulkImportHFile(self, dataframe, schema_table_name, options)\n",
      " |      Bulk Import HFile from a dataframe into a schema.table\n",
      " |      :param dataframe: Input data\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param options: Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required\n",
      " |  \n",
      " |  bulkImportHFileWithRdd(self, rdd, schema, schema_table_name, options)\n",
      " |      Bulk Import HFile from an rdd into a schema.table\n",
      " |      :param rdd: Input data\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param options: Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required\n",
      " |  \n",
      " |  createDataFrame(self, rdd, schema)\n",
      " |      Creates a dataframe from a given rdd and schema.\n",
      " |      \n",
      " |      :param rdd: Input data\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |  \n",
      " |  createTable(self, dataframe, schema_table_name, primary_keys=None, create_table_options=None, to_upper=False, drop_table=False)\n",
      " |      Creates a schema.table from a dataframe\n",
      " |      :param dataframe: The Spark DataFrame to base the table off\n",
      " |      :param schema_table_name: str The schema.table to create\n",
      " |      :param primary_keys: List[str] the primary keys. Default None\n",
      " |      :param create_table_options: str The additional table-level SQL options default None\n",
      " |      :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      " |                          If False, the table will be created with lower case columns. Default False\n",
      " |      :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists,\n",
      " |                         the function will throw an exception.\n",
      " |  \n",
      " |  createTableWithSchema(self, schema_table_name, schema, keys=None, create_table_options=None)\n",
      " |      Creates a schema.table from a schema\n",
      " |      :param schema_table_name: str The schema.table to create\n",
      " |      :param schema: (StructType) The schema that describes the columns of the table\n",
      " |      :param keys: List[str] The primary keys. Default None\n",
      " |      :param create_table_options: str The additional table-level SQL options. Default None\n",
      " |  \n",
      " |  delete(self, dataframe, schema_table_name)\n",
      " |      Delete records in a dataframe based on joining by primary keys from the data frame.\n",
      " |      Be careful with column naming and case sensitivity.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to delete\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  deleteWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Delete records using an rdd based on joining by primary keys from the rdd.\n",
      " |      Be careful with column naming and case sensitivity.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD containing the primary keys you would like to delete from the table\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  df(self, sql)\n",
      " |      Return a Spark Dataframe from the results of a Splice Machine SQL Query\n",
      " |      \n",
      " |      :param sql: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return: A Spark DataFrame containing the results\n",
      " |  \n",
      " |  dropTable(self, schema_and_or_table_name, table_name=None)\n",
      " |      Drop a specified table.\n",
      " |      \n",
      " |      Call it like:\n",
      " |          dropTable('schemaName.tableName')\n",
      " |      Or:\n",
      " |          dropTable('schemaName', 'tableName')\n",
      " |      \n",
      " |      :param schema_and_or_table_name: (string) Pass the schema name in this param when passing the table_name param,\n",
      " |        or pass schemaName.tableName in this param without passing the table_name param\n",
      " |      :param table_name: (optional) (string) Table Name, used when schema_and_or_table_name contains only the schema name\n",
      " |  \n",
      " |  execute(self, query_string)\n",
      " |      execute a query\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return:\n",
      " |  \n",
      " |  executeUpdate(self, query_string)\n",
      " |      execute a dml query:(update,delete,drop,etc)\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return:\n",
      " |  \n",
      " |  export(self, dataframe, location, compression=False, replicationCount=1, fileEncoding=None, fieldSeparator=None, quoteCharacter=None)\n",
      " |      Export a dataFrame in CSV\n",
      " |      :param dataframe:\n",
      " |      :param location: Destination directory\n",
      " |      :param compression: Whether to compress the output or not\n",
      " |      :param replicationCount:  Replication used for HDFS write\n",
      " |      :param fileEncoding: fileEncoding or null, defaults to UTF-8\n",
      " |      :param fieldSeparator: fieldSeparator or null, defaults to ','\n",
      " |      :param quoteCharacter: quoteCharacter or null, defaults to '\"'\n",
      " |      :return:\n",
      " |  \n",
      " |  exportBinary(self, dataframe, location, compression, e_format)\n",
      " |      Export a dataFrame in binary format\n",
      " |      :param dataframe:\n",
      " |      :param location: Destination directory\n",
      " |      :param compression: Whether to compress the output or not\n",
      " |      :param e_format: Binary format to be used, currently only 'parquet' is supported\n",
      " |      :return:\n",
      " |  \n",
      " |  getConnection(self)\n",
      " |      Return a connection to the database\n",
      " |  \n",
      " |  getSchema(self, schema_table_name)\n",
      " |      Return the schema via JDBC.\n",
      " |      \n",
      " |      :param schema_table_name: (DF) Table name\n",
      " |  \n",
      " |  insert(self, dataframe, schema_table_name, to_upper=False)\n",
      " |      Insert a dataframe into a table (schema.table).\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to insert\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the DF\n",
      " |      :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      " |                          If False, the table will be created with lower case columns. Default False\n",
      " |  \n",
      " |  insertRdd(self, rdd, schema, schema_table_name)\n",
      " |      Insert an rdd into a table (schema.table).\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to insert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the RDD\n",
      " |  \n",
      " |  insertRddWithStatus(self, rdd, schema, schema_table_name, statusDirectory, badRecordsAllowed)\n",
      " |      Insert an rdd into a table (schema.table) while tracking and limiting records that fail to insert.\n",
      " |      The status directory and number of badRecordsAllowed allow for duplicate primary keys to be\n",
      " |      written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n",
      " |      to the status directory.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to insert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the dataframe\n",
      " |      :param statusDirectory The status directory where bad records file will be created\n",
      " |      :param badRecordsAllowed The number of bad records are allowed. -1 for unlimited\n",
      " |  \n",
      " |  insertWithStatus(self, dataframe, schema_table_name, statusDirectory, badRecordsAllowed)\n",
      " |      Insert a dataframe into a table (schema.table) while tracking and limiting records that fail to insert.\n",
      " |      The status directory and number of badRecordsAllowed allow for duplicate primary keys to be\n",
      " |      written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written\n",
      " |      to the status directory.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to insert\n",
      " |      :param schema_table_name: (string) The table in which you would like to insert the dataframe\n",
      " |      :param statusDirectory The status directory where bad records file will be created\n",
      " |      :param badRecordsAllowed The number of bad records are allowed. -1 for unlimited\n",
      " |  \n",
      " |  internalDf(self, query_string)\n",
      " |      SQL to Dataframe translation.  (Lazy)\n",
      " |      Runs the query inside Splice Machine and sends the results to the Spark Adapter app\n",
      " |      :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 &gt; 3)\n",
      " |      :return: pyspark dataframe contains the result of query_string\n",
      " |  \n",
      " |  internalRdd(self, schema_table_name, column_projection=None)\n",
      " |      Table with projections in Splice mapped to an RDD.\n",
      " |      :param schema_table_name: (string) Accessed table\n",
      " |      :param column_projection: (list of strings) Names of selected columns\n",
      " |      :return RDD[Row] with the result of the projection\n",
      " |  \n",
      " |  rdd(self, schema_table_name, column_projection=None)\n",
      " |      Table with projections in Splice mapped to an RDD.\n",
      " |      :param schema_table_name: (string) Accessed table\n",
      " |      :param column_projection: (list of strings) Names of selected columns\n",
      " |      :return RDD[Row] with the result of the projection\n",
      " |  \n",
      " |  replaceDataframeSchema(self, dataframe, schema_table_name)\n",
      " |      Returns a dataframe with all column names replaced with the proper string case from the DB table\n",
      " |      :param dataframe: A dataframe with column names to convert\n",
      " |      :param schema_table_name: The schema.table with the correct column cases to pull from the database\n",
      " |  \n",
      " |  splitAndInsert(self, dataframe, schema_table_name, sample_fraction)\n",
      " |      Sample the dataframe, split the table, and insert a dataFrame into a schema.table.\n",
      " |      This corresponds to an insert into from select statement\n",
      " |      :param dataframe: Input data\n",
      " |      :param schema_table_name: Full table name in the format of \"schema.table\"\n",
      " |      :param sample_fraction: (float) A value between 0 and 1 that specifies the percentage of data in the dataFrame\n",
      " |          that should be sampled to determine the splits.\n",
      " |          For example, specify 0.005 if you want 0.5% of the data sampled.\n",
      " |  \n",
      " |  tableExists(self, schema_and_or_table_name, table_name=None)\n",
      " |      Check whether or not a table exists\n",
      " |      \n",
      " |      Call it like:\n",
      " |          tableExists('schemaName.tableName')\n",
      " |      Or:\n",
      " |          tableExists('schemaName', 'tableName')\n",
      " |      \n",
      " |      :param schema_and_or_table_name: (string) Pass the schema name in this param when passing the table_name param,\n",
      " |        or pass schemaName.tableName in this param without passing the table_name param\n",
      " |      :param table_name: (optional) (string) Table Name, used when schema_and_or_table_name contains only the schema name\n",
      " |  \n",
      " |  toUpper(self, dataframe)\n",
      " |      Returns a dataframe with all of the columns in uppercase\n",
      " |      :param dataframe: The dataframe to convert to uppercase\n",
      " |  \n",
      " |  truncateTable(self, schema_table_name)\n",
      " |      truncate a table\n",
      " |      :param schema_table_name: the full table name in the format \"schema.table_name\" which will be truncated\n",
      " |      :return:\n",
      " |  \n",
      " |  update(self, dataframe, schema_table_name)\n",
      " |      Update data from a dataframe for a specified schema_table_name (schema.table).\n",
      " |      The keys are required for the update and any other columns provided will be updated\n",
      " |      in the rows.\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to update\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  updateWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Update data from an rdd for a specified schema_table_name (schema.table).\n",
      " |      The keys are required for the update and any other columns provided will be updated\n",
      " |      in the rows.\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to use for updating the table\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) Splice Machine Table\n",
      " |  \n",
      " |  upsert(self, dataframe, schema_table_name)\n",
      " |      Upsert the data from a dataframe into a table (schema.table).\n",
      " |      \n",
      " |      :param dataframe: (DF) The dataframe you would like to upsert\n",
      " |      :param schema_table_name: (string) The table in which you would like to upsert the RDD\n",
      " |  \n",
      " |  upsertWithRdd(self, rdd, schema, schema_table_name)\n",
      " |      Upsert the data from an RDD into a table (schema.table).\n",
      " |      \n",
      " |      :param rdd: (RDD) The RDD you would like to upsert\n",
      " |      :param schema: (StructType) The schema of the rows in the RDD\n",
      " |      :param schema_table_name: (string) The table in which you would like to upsert the RDD\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "</code>, <code>splicemachine.spark.context</code>, <code>class ExtPySpliceContext(PySpliceContext)\n",
      "|  ExtPySpliceContext(sparkSession, JDBC_URL=None, kafkaServers='localhost:9092', kafkaPollTimeout=20000, _unit_testing=False)\n",
      "|  \n",
      "|  This class implements a SplicemachineContext object from com.splicemachine.spark2\n",
      "|  \n",
      "|  Method resolution order:\n",
      "|      ExtPySpliceContext\n",
      "|      PySpliceContext\n",
      "|      builtins.object\n",
      "</code>, <code>splicemachine.mlflow_support</code>, <code>USAGE\n",
      "   from splicemachine.mlflow_support import *\n",
      "   mlflow.register_splice_context(splice) # The Native Spark Datasource (or External)\n",
      "   mlflow.start_run()\n",
      "   mlflow.log_param(\"my\", \"param\")\n",
      "   mlflow.log_metric(\"score\", 100)\n",
      "   mlflow.end_run()\n",
      "You can also use syntax like this:\n",
      "   with mlflow.start_run() as run:\n",
      "        ...\n",
      "which automatically terminates the run at the end of the block.\n",
      "\n",
      "For the lower level client-side API, see `mlflow.client` available functions.\n",
      "\n",
      "ADDITIONAL FUNCTIONS\n",
      "\n",
      "register_splice_context(splice_context)\n",
      "  Register a Splice Context for Spark/Database operations. This is required before most other functions become available.\n",
      "  (artifact storage, for example)\n",
      "  :param splice_context:  splice context to input\n",
      "\n",
      "timer(timer_name, param=True)\n",
      "  Context manager for logging\n",
      "  :param timer_name:\n",
      "  :param param: whether or not to log the timer as a param (default=True). If false, logs as metric.\n",
      "  :return:\n",
      "  \n",
      "  Usage:\n",
      "  mlflow.start_run()\n",
      "  with mlflow.timer('run_time', param=False): # Log timer as param or metric\n",
      "      mlflow.log_param(\"my\", \"param\")\n",
      "      mlflow.log_metric(\"score\", 100)\n",
      "      mlflow.end_run()\n",
      "  You can also use the syntax like this:\n",
      "  with mlflow.start_run():\n",
      "      with mlflow.timer('run_time', param=False): # Log timer as param or metric\n",
      "          mlflow.log_param(\"my\", \"param\")\n",
      "          mlflow.log_metric(\"score\", 100)\n",
      "\n",
      "get_run_ids_by_name(run_name, experiment_id=None)\n",
      "  Gets a run id from the run name. If there are multiple runs with the same name, all run IDs are returned\n",
      "  :param run_name: The name of the run\n",
      "  :param experiment_id: The experiment to search in. If None, all experiments are searched\n",
      "  :return: List of run ids\n",
      "  \n",
      "current_run_id()\n",
      "  Retrieve the current run id\n",
      "  :return: the current run id\n",
      "  \n",
      "current_exp_id()\n",
      "  Retrieve the current exp id\n",
      "  :return: the current experiment id\n",
      "  \n",
      "lp(key, value):\n",
      "  Add a shortcut for logging parameters in MLFlow.\n",
      "  Accessible from mlflow.lp\n",
      "  :param key: key for the parameter\n",
      "  :param value: value for the parameter\n",
      "  \n",
      "lm(key, value, step=None):\n",
      "  Add a shortcut for logging metrics in MLFlow.\n",
      "  Accessible from mlflow.lm\n",
      "  :param key: key for the parameter\n",
      "  :param value: value for the parameter\n",
      "  \n",
      "log_model(model, name='model'):\n",
      "  Log a fitted spark pipeline/model or H2O model\n",
      "  :param model: (PipelineModel or Model) is the fitted Spark Model/Pipeline or H2O model to store\n",
      "      with the current run\n",
      "  :param name: (str) the run relative name to store the model under\n",
      "\n",
      "log_pipeline_stages(pipeline):\n",
      "  \"\"\"\n",
      "  Log the pipeline stages as params for the run. Currently only Spark Pipelines are supported\n",
      "  :param pipeline: fitted/unitted (spark) pipeline    \n",
      "  \n",
      "log_feature_transformations(unfit_pipeline):\n",
      "  Log feature transformations for an unfit pipeline\n",
      "  Logs --&gt; feature movement through the pipeline.\n",
      "  Currently only Spark pipelines are supported.\n",
      "  :param unfit_pipeline: unfit pipeline to log\n",
      "  \n",
      "log_model_params(pipeline_or_model):\n",
      "  Log the parameters of a fitted model or a\n",
      "  model part of a fitted pipeline. Currently only Spark models/Pipelines are supported\n",
      "  :param pipeline_or_model: fitted pipeline/fitted model\n",
      "  \n",
      "download_artifact(name, local_path, run_id=None):\n",
      "  Download the artifact at the given\n",
      "  run id (active default) + name\n",
      "  to the local path\n",
      "  :param name: (str) artifact name to load\n",
      "    (with respect to the run)\n",
      "  :param local_path: (str) local path to download the\n",
      "    model to. This path MUST include the file extension\n",
      "  :param run_id: (str) the run id to download the artifact\n",
      "    from. Defaults to active run\n",
      "    \n",
      "get_model_name(run_id):\n",
      "  Gets the model name associated with a run or None\n",
      "  :param run_id:\n",
      "  :return: str or None\n",
      "  \n",
      "load_model(run_id=None, name=None):\n",
      "  Download a model from database\n",
      "  and load it into Spark\n",
      "  :param run_id: the id of the run to get a model from\n",
      "      (the run must have an associated model with it named spark_model)\n",
      "  :param name: the name of the model in the database\n",
      "  \n",
      "login_director(username, password):\n",
      "  Authenticate into the MLManager Director\n",
      "  :param username: database username\n",
      "  :param password: database password\n",
      "  \n",
      "deploy_aws(app_name, region='us-east-2', instance_type='ml.m5.xlarge',\n",
      "              run_id=None, instance_count=1, deployment_mode='replace'):\n",
      "  \"\"\"\n",
      "  Queue Job to deploy a run to sagemaker with the\n",
      "  given run id (found in MLFlow UI or through search API)\n",
      "  :param run_id: the id of the run to deploy. Will default to the current\n",
      "      run id.\n",
      "  :param app_name: the name of the app in sagemaker once deployed\n",
      "  :param region: the sagemaker region to deploy to (us-east-2,\n",
      "      us-west-1, us-west-2, eu-central-1 supported)\n",
      "  :param instance_type: the EC2 Sagemaker instance type to deploy on\n",
      "      (ml.m4.xlarge supported)\n",
      "  :param instance_count: the number of instances to load balance predictions\n",
      "      on\n",
      "  :param deployment_mode: the method to deploy; create=application will fail\n",
      "      if an app with the name specified already exists; replace=application\n",
      "      in sagemaker will be replaced with this one if app already exists;\n",
      "      add=add the specified model to a prexisting application (not recommended)\n",
      "  \n",
      "deploy_azure(endpoint_name, resource_group, workspace, run_id=None, region='East US',\n",
      "                cpu_cores=0.1, allocated_ram=0.5, model_name=None):\n",
      "  Deploy a given run to AzureML.\n",
      "  :param endpoint_name: (str) the name of the endpoint in AzureML when deployed to\n",
      "      Azure Container Services. Must be unique.\n",
      "  :param resource_group: (str) Azure Resource Group for model. Automatically created if\n",
      "      it doesn't exist.\n",
      "  :param workspace: (str) the AzureML workspace to deploy the model under.\n",
      "      Will be created if it doesn't exist\n",
      "  :param run_id: (str) if specified, will deploy a previous run (\n",
      "      must have an spark model logged). Otherwise, will default to the active run\n",
      "  :param region: (str) AzureML Region to deploy to: Can be East US, East US 2, Central US,\n",
      "      West US 2, North Europe, West Europe or Japan East\n",
      "  :param cpu_cores: (float) Number of CPU Cores to allocate to the instance.\n",
      "      Can be fractional. Default=0.1\n",
      "  :param allocated_ram: (float) amount of RAM, in GB, allocated to the container.\n",
      "      Default=0.5\n",
      "  :param model_name: (str) If specified, this will be the name of the model in AzureML.\n",
      "      Otherwise, the model name will be randomly generated.\n",
      "      \n",
      "deploy_db(db_schema_name, db_table_name, run_id, primary_key=None, df=None, create_model_table=False, model_cols=None, classes=None, sklearn_args={}, verbose=False, pred_threshold=None, replace=False) -&gt; None\n",
      "  Function to deploy a trained (currently Spark, Sklearn or H2O) model to the Database.\n",
      "  This creates 2 tables: One with the features of the model, and one with the prediction and metadata.\n",
      "  They are linked with a column called MOMENT_ID\n",
      "  \n",
      "  :param db_schema_name: (str) the schema name to deploy to. If None, the currently set schema will be used.\n",
      "  :param db_table_name: (str) the table name to deploy to. If none, the run_id will be used for the table name(s)\n",
      "  :param run_id: (str) The run_id to deploy the model on. The model associated with this run will be deployed\n",
      "  \n",
      "  \n",
      "  OPTIONAL PARAMETERS:\n",
      "  :param primary_key: (List[Tuple[str, str]]) List of column + SQL datatype to use for the primary/composite key.\n",
      "                      If you are deploying to a table that already exists, this primary/composite key must exist in the table\n",
      "                      If you are creating the table in this function, you MUST pass in a primary key\n",
      "  :param df: (Spark or Pandas DF) The dataframe used to train the model\n",
      "              NOTE: this dataframe should NOT be transformed by the model. The columns in this df are the ones\n",
      "              that will be used to create the table.\n",
      "  :param create_model_table: Whether or not to create the table from the dataframe. Default false. This\n",
      "                              Will ONLY be used if the table does not exist and a dataframe is passed in\n",
      "  :param predictor_cols: (List[str]) The columns from the table to use for the model. If None, all columns in the table\n",
      "                                      will be passed to the model. If specified, the columns will be passed to the model\n",
      "                                      IN THAT ORDER. The columns passed here must exist in the table.\n",
      "  :param classes: (List[str]) The classes (prediction labels) for the model being deployed.\n",
      "                  NOTE: If not supplied, the table will have default column names for each class\n",
      "  :param sklearn_args: (dict{str: str}) Prediction options for sklearn models\n",
      "                      Available key value options:\n",
      "                      'predict_call': 'predict', 'predict_proba', or 'transform'\n",
      "                                                                     - Determines the function call for the model\n",
      "                                                                     If blank, predict will be used\n",
      "                                                                     (or transform if model doesn't have predict)\n",
      "                      'predict_args': 'return_std' or 'return_cov' - For Bayesian and Gaussian models\n",
      "                                                                       Only one can be specified\n",
      "                      If the model does not have the option specified, it will be ignored.\n",
      "  :param verbose: (bool) Whether or not to print out the queries being created. Helpful for debugging\n",
      "  :param pred_threshold: (double) A prediction threshold for *Keras* binary classification models\n",
      "                          If the model type isn't Keras, this parameter will be ignored\n",
      "                          NOTE: If the model type is Keras, the output layer has 1 node, and pred_threshold is None,\n",
      "                                you will NOT receive a class prediction, only the output of the final layer (like model.predict()).\n",
      "                                If you want a class prediction\n",
      "                                for your binary classification problem, you MUST pass in a threshold.\n",
      "  :param replace: (bool) whether or not to replace a currently existing model. This param does not yet work\n",
      "  \n",
      "  \n",
      "  This function creates the following:\n",
      "  IF you are creating the table from the dataframe:\n",
      "      * The model table where run_id is the run_id passed in (or the current active run_id\n",
      "          This will have a column for each feature in the feature vector. It will also contain:\n",
      "          USER which is the current user who made the request\n",
      "          EVAL_TIME which is the CURRENT_TIMESTAMP\n",
      "          the PRIMARY KEY column same as the DATA table to link predictions to rows in the table (primary key)\n",
      "          PREDICTION. The prediction of the model. If the :classes: param is not filled in, this will be default values for classification models\n",
      "          A column for each class of the predictor with the value being the probability/confidence of the model if applicable\n",
      "  IF you are deploying to an existing table:\n",
      "      * The table will be altered to include\n",
      "  \n",
      "  * A trigger that runs on (after) insertion to the data table that runs an INSERT into the prediction table,\n",
      "      calling the PREDICT function, passing in the row of data as well as the schema of the dataset, and the run_id of the model to run\n",
      "  * A trigger that runs on (after) insertion to the prediction table that calls an UPDATE to the row inserted,\n",
      "      parsing the prediction probabilities and filling in proper column values\n",
      "      \n",
      "get_deployed_models() -&gt; PandasDF:\n",
      "  \"\"\"\n",
      "  Get the currently deployed models in the database\n",
      "  :return: Pandas df\n",
      "</code>, <code>splicemachine.stats</code>, <code>    class DecisionTreeVisualizer(builtins.object)\n",
      "     |  Visualize a decision tree, either in code like format, or graphviz\n",
      "     |  \n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  add_node(dot, parent, node_hash, root, realroot=False)\n",
      "     |      Traverse through the .debugString json and generate a graphviz tree\n",
      "     |      :param dot: dot file object\n",
      "     |      :param parent: not used currently\n",
      "     |      :param node_hash: unique node id\n",
      "     |      :param root: the root of tree\n",
      "     |      :param realroot: whether or not it is the real root, or a recursive root\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  feature_importance(spark, model, dataset, featuresCol='features')\n",
      "     |      Return a dataframe containing the relative importance of each feature\n",
      "     |      :param model:\n",
      "     |      :param dataframe:\n",
      "     |      :param featureCol:\n",
      "     |      :return: dataframe containing importance\n",
      "     |  \n",
      "     |  parse(lines)\n",
      "     |      Lines in debug string\n",
      "     |      :param lines:\n",
      "     |      :return: block json\n",
      "     |  \n",
      "     |  replacer(string, bad, good)\n",
      "     |      Replace every string in \"bad\" with the corresponding string in \"good\"\n",
      "     |      :param string: string to replace in\n",
      "     |      :param bad: array of strings to replace\n",
      "     |      :param good: array of strings to replace with\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  tree_json(tree)\n",
      "     |      Generate a JSON representation of a decision tree\n",
      "     |      :param tree: tree debug string\n",
      "     |      :return: json\n",
      "     |  \n",
      "     |  visualize(model, feature_column_names, label_names, size=None, horizontal=False, tree_name='tree', visual=False)\n",
      "     |      Visualize a decision tree, either in a code like format, or graphviz\n",
      "     |      :param model: the fitted decision tree classifier\n",
      "     |      :param feature_column_names: (List[str]) column names for features\n",
      "     |             You can access these feature names by using your VectorAssembler (in PySpark) and calling it's .getInputCols() function\n",
      "     |      :param label_names: (List[str]) labels vector (below avg, above avg)\n",
      "     |      :param size: tuple(int,int) The size of the graph. If unspecified, graphviz will automatically assign a size\n",
      "     |      :param horizontal: (Bool) if the tree should be rendered horizontally\n",
      "     |      :param tree_name: the name you would like to call the tree\n",
      "     |      :param visual: bool, true if you want a graphviz pdf containing your file\n",
      "     |      :return dot: The graphvis object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class IndReconstructer(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  IndReconstructer(inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  Transformer to reconstruct String Index from OneHotDummy Columns. This can be used as a part of a Pipeline Ojbect\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: Transformed PySpark Dataframe With Original String Indexed Variables\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndReconstructer\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputCol=None, outputCol=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  setParams(self, inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class MarkovChain(builtins.object)\n",
      "     |  MarkovChain(transition_prob)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, transition_prob)\n",
      "     |      Initialize the MarkovChain instance.\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transition_prob: dict\n",
      "     |          A dict object representing the transition\n",
      "     |          probabilities in Markov Chain.\n",
      "     |          Should be of the form:\n",
      "     |              {'state1': {'state1': 0.1, 'state2': 0.4},\n",
      "     |               'state2': {...}}\n",
      "     |  \n",
      "     |  generate_states(self, current_state, no=10, last=True)\n",
      "     |      Generates the next states of the system.\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      current_state: str\n",
      "     |          The state of the current random variable.\n",
      "     |      no: int\n",
      "     |          The number of future states to generate.\n",
      "     |      last: bool\n",
      "     |          Do we want to return just the last value\n",
      "     |  \n",
      "     |  get_max_num_steps(self)\n",
      "     |  \n",
      "     |  next_state(self, current_state)\n",
      "     |      Returns the state of the random variable at the next time\n",
      "     |      instance.\n",
      "     |      :param current_state: The current state of the system.\n",
      "     |      :raises: Exception if random choice fails\n",
      "     |      :return: next state\n",
      "     |  \n",
      "     |  rep_states(self, current_state, no=10, num_reps=10)\n",
      "     |      running generate states a bunch of times and returning the final state that happens the most\n",
      "     |      Arguments:\n",
      "     |          current_state str -- The state of the current random variable\n",
      "     |          no int -- number of time steps in the future to run\n",
      "     |          num_reps int -- number of times to run the simultion forward\n",
      "     |      Returns:\n",
      "     |          state -- the most commonly reached state at the end of these runs\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OneHotDummies(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  OneHotDummies(inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  Transformer to generate dummy columns for categorical variables as a part of a preprocessing pipeline\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Classes\n",
      "     |  :param HasInputCol: Inherited Classes\n",
      "     |  :param HasOutputCol: Inherited Classes\n",
      "     |  :return: pyspark DataFrame\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotDummies\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, inputCol=None, outputCol=None)\n",
      "     |      Assigns variables to parameters passed\n",
      "     |      :param inputCol: Sparse vector returned by OneHotEncoders, defaults to None\n",
      "     |      :param outputCol: string base to append to output columns names, defaults to None\n",
      "     |  \n",
      "     |  getOutCols(self)\n",
      "     |  \n",
      "     |  setParams(self, inputCol=None, outputCol=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OverSampleCrossValidator(pyspark.ml.tuning.CrossValidator)\n",
      "     |  OverSampleCrossValidator(estimator, estimatorParamMaps, evaluator, numFolds=3, seed=None, parallelism=3, collectSubModels=False, labelCol='label', altEvaluators=None, overSample=True)\n",
      "     |  \n",
      "     |  Class to perform Cross Validation model evaluation while over-sampling minority labels.\n",
      "     |  Example:\n",
      "     |  -------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.classification import LogisticRegression\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.linalg import Vectors\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import OverSampleCrossValidator\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; dataset = spark.createDataFrame(\n",
      "     |  ...      [(Vectors.dense([0.0]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.5]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.4]), 1.0),\n",
      "     |  ...       (Vectors.dense([0.6]), 1.0),\n",
      "     |  ...       (Vectors.dense([1.0]), 1.0)] * 10,\n",
      "     |  ...      [\"features\", \"label\"])\n",
      "     |  &gt;&gt;&gt; lr = LogisticRegression()\n",
      "     |  &gt;&gt;&gt; grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
      "     |  &gt;&gt;&gt; PRevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderPR')\n",
      "     |  &gt;&gt;&gt; AUCevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
      "     |  &gt;&gt;&gt; ACCevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
      "     |  &gt;&gt;&gt; cv = OverSampleCrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=AUCevaluator, altEvaluators = [PRevaluator, ACCevaluator],parallelism=2,seed = 1234)\n",
      "     |  &gt;&gt;&gt; cvModel = cv.fit(dataset)\n",
      "     |  &gt;&gt;&gt; print(cvModel.avgMetrics)\n",
      "     |  [(0.5, [0.5888888888888888, 0.3888888888888889]), (0.806878306878307, [0.8556863149300125, 0.7055555555555556])]\n",
      "     |  &gt;&gt;&gt; print(AUCevaluator.evaluate(cvModel.transform(dataset)))\n",
      "     |  0.8333333333333333\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OverSampleCrossValidator\n",
      "     |      pyspark.ml.tuning.CrossValidator\n",
      "     |      pyspark.ml.base.Estimator\n",
      "     |      pyspark.ml.tuning._CrossValidatorParams\n",
      "     |      pyspark.ml.tuning._ValidatorParams\n",
      "     |      pyspark.ml.param.shared.HasSeed\n",
      "     |      pyspark.ml.param.shared.HasParallelism\n",
      "     |      pyspark.ml.param.shared.HasCollectSubModels\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, estimatorParamMaps, evaluator, numFolds=3, seed=None, parallelism=3, collectSubModels=False, labelCol='label', altEvaluators=None, overSample=True)\n",
      "     |      Initialize Self\n",
      "     |      :param estimator: Machine Learning Model, defaults to None\n",
      "     |      :param estimatorParamMaps: paramMap to search, defaults to None\n",
      "     |      :param evaluator: primary model evaluation metric, defaults to None\n",
      "     |      :param numFolds: number of folds to perform, defaults to 3\n",
      "     |      :param seed: random state, defaults to None\n",
      "     |      :param parallelism: number of threads, defaults to 1\n",
      "     |      :param collectSubModels: to return submodels, defaults to False\n",
      "     |      :param labelCol: target variable column label, defaults to 'label'\n",
      "     |      :param altEvaluators: additional metrics to evaluate, defaults to None\n",
      "     |                           If passed, the metrics of the alternate evaluators are accessed in the CrossValidatorModel.avgMetrics attribute\n",
      "     |      :param overSample: Boolean: to perform oversampling of minority labels, defaults to True\n",
      "     |  \n",
      "     |  getAltEvaluators(self)\n",
      "     |  \n",
      "     |  getLabel(self)\n",
      "     |  \n",
      "     |  getOversample(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.tuning.CrossValidator:\n",
      "     |  \n",
      "     |  copy(self, extra=None)\n",
      "     |      Creates a copy of this instance with a randomly generated uid\n",
      "     |      and some extra params. This copies creates a deep copy of\n",
      "     |      the embedded paramMap, and copies the embedded and extra parameters over.\n",
      "     |      \n",
      "     |      :param extra: Extra parameters to copy to the new instance\n",
      "     |      :return: Copy of this instance\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setCollectSubModels(self, value)\n",
      "     |      Sets the value of :py:attr:`collectSubModels`.\n",
      "     |  \n",
      "     |  setEstimator(self, value)\n",
      "     |      Sets the value of :py:attr:`estimator`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setEstimatorParamMaps(self, value)\n",
      "     |      Sets the value of :py:attr:`estimatorParamMaps`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setEvaluator(self, value)\n",
      "     |      Sets the value of :py:attr:`evaluator`.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.0.0\n",
      "     |  \n",
      "     |  setNumFolds(self, value)\n",
      "     |      Sets the value of :py:attr:`numFolds`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setParallelism(self, value)\n",
      "     |      Sets the value of :py:attr:`parallelism`.\n",
      "     |  \n",
      "     |  setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3, seed=None, parallelism=1, collectSubModels=False)\n",
      "     |      setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, numFolds=3,                  seed=None, parallelism=1, collectSubModels=False):\n",
      "     |      Sets params for cross validator.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  setSeed(self, value)\n",
      "     |      Sets the value of :py:attr:`seed`.\n",
      "     |  \n",
      "     |  write(self)\n",
      "     |      Returns an MLWriter instance for this ML instance.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from pyspark.ml.tuning.CrossValidator:\n",
      "     |  \n",
      "     |  read() from builtins.type\n",
      "     |      Returns an MLReader instance for this class.\n",
      "     |      \n",
      "     |      .. versionadded:: 2.3.0\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class OverSampler(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  OverSampler(labelCol=None, strategy='auto', randomState=None)\n",
      "     |  \n",
      "     |  Transformer to oversample datapoints with minority labels\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: PySpark Dataframe with labels in approximately equal ratios\n",
      "     |  Example:\n",
      "     |  -------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql import functions as F\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from pyspark.stats.linalg import Vectors\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import OverSampler\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; df = spark.createDataFrame(\n",
      "     |  ...      [(Vectors.dense([0.0]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.5]), 0.0),\n",
      "     |  ...       (Vectors.dense([0.4]), 1.0),\n",
      "     |  ...       (Vectors.dense([0.6]), 1.0),\n",
      "     |  ...       (Vectors.dense([1.0]), 1.0)] * 10,\n",
      "     |  ...      [\"features\", \"Class\"])\n",
      "     |  &gt;&gt;&gt; df.groupBy(F.col(\"Class\")).count().orderBy(\"count\").show()\n",
      "     |  +-----+-----+\n",
      "     |  |Class|count|\n",
      "     |  +-----+-----+\n",
      "     |  |  0.0|   20|\n",
      "     |  |  1.0|   30|\n",
      "     |  +-----+-----+\n",
      "     |  &gt;&gt;&gt; oversampler = OverSampler(labelCol = \"Class\", strategy = \"auto\")\n",
      "     |  &gt;&gt;&gt; oversampler.transform(df).groupBy(\"Class\").count().show()\n",
      "     |  +-----+-----+\n",
      "     |  |Class|count|\n",
      "     |  +-----+-----+\n",
      "     |  |  0.0|   29|\n",
      "     |  |  1.0|   30|\n",
      "     |  +-----+-----+\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OverSampler\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, labelCol=None, strategy='auto', randomState=None)\n",
      "     |      Initialize self\n",
      "     |      :param labelCol: Label Column name, defaults to None\n",
      "     |      :param strategy: defaults to \"auto\", strategy to resample the dataset:\n",
      "     |                      • Only currently supported for \"auto\" Corresponds to random samples with repleaement\n",
      "     |      :param randomState: sets the seed of sample algorithm\n",
      "     |  \n",
      "     |  setParams(self, labelCol=None, strategy='auto')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  deserializeFromBundle(path)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class Rounder(pyspark.ml.base.Transformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.DefaultParamsReadable, pyspark.ml.util.DefaultParamsWritable)\n",
      "     |  Rounder(predictionCol='prediction', labelCol='label', clipPreds=True, maxLabel=None, minLabel=None)\n",
      "     |  \n",
      "     |  Transformer to round predictions for ordinal regression\n",
      "     |  Follows: https://spark.apache.org/docs/latest/ml-pipeline.html#transformers\n",
      "     |  :param Transformer: Inherited Class\n",
      "     |  :param HasInputCol: Inherited Class\n",
      "     |  :param HasOutputCol: Inherited Class\n",
      "     |  :return: Transformed Dataframe with rounded predictionCol\n",
      "     |  Example:\n",
      "     |  --------\n",
      "     |  &gt;&gt;&gt; from pyspark.sql.session import SparkSession\n",
      "     |  &gt;&gt;&gt; from splicemachine.stats.stats import Rounder\n",
      "     |  &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n",
      "     |  &gt;&gt;&gt; dataset = spark.createDataFrame(\n",
      "     |  ...      [(0.2, 0.0),\n",
      "     |  ...       (1.2, 1.0),\n",
      "     |  ...       (1.6, 2.0),\n",
      "     |  ...       (1.1, 0.0),\n",
      "     |  ...       (3.1, 0.0)],\n",
      "     |  ...      [\"prediction\", \"label\"])\n",
      "     |  &gt;&gt;&gt; dataset.show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.2|  0.0|\n",
      "     |  |       1.2|  1.0|\n",
      "     |  |       1.6|  2.0|\n",
      "     |  |       1.1|  0.0|\n",
      "     |  |       3.1|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  &gt;&gt;&gt; rounder = Rounder(predictionCol = \"prediction\", labelCol = \"label\", clipPreds = True)\n",
      "     |  &gt;&gt;&gt; rounder.transform(dataset).show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.0|  0.0|\n",
      "     |  |       1.0|  1.0|\n",
      "     |  |       2.0|  2.0|\n",
      "     |  |       1.0|  0.0|\n",
      "     |  |       2.0|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  &gt;&gt;&gt; rounderNoClip = Rounder(predictionCol = \"prediction\", labelCol = \"label\", clipPreds = False)\n",
      "     |  &gt;&gt;&gt; rounderNoClip.transform(dataset).show()\n",
      "     |  +----------+-----+\n",
      "     |  |prediction|label|\n",
      "     |  +----------+-----+\n",
      "     |  |       0.0|  0.0|\n",
      "     |  |       1.0|  1.0|\n",
      "     |  |       2.0|  2.0|\n",
      "     |  |       1.0|  0.0|\n",
      "     |  |       3.0|  0.0|\n",
      "     |  +----------+-----+\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Rounder\n",
      "     |      pyspark.ml.base.Transformer\n",
      "     |      pyspark.ml.param.shared.HasInputCol\n",
      "     |      pyspark.ml.param.shared.HasOutputCol\n",
      "     |      pyspark.ml.param.Params\n",
      "     |      pyspark.ml.util.Identifiable\n",
      "     |      pyspark.ml.util.DefaultParamsReadable\n",
      "     |      pyspark.ml.util.MLReadable\n",
      "     |      pyspark.ml.util.DefaultParamsWritable\n",
      "     |      pyspark.ml.util.MLWritable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, predictionCol='prediction', labelCol='label', clipPreds=True, maxLabel=None, minLabel=None)\n",
      "     |      initialize self\n",
      "     |      :param predictionCol: column containing predictions, defaults to \"prediction\"\n",
      "     |      :param labelCol: column containing labels, defaults to \"label\"\n",
      "     |      :param clipPreds: clip all predictions above a specified maximum value\n",
      "     |      :param maxLabel: optional: the maximum value for the prediction column, otherwise uses the maximum of the labelCol, defaults to None\n",
      "     |      :param minLabel: optional: the minimum value for the prediction column, otherwise uses the maximum of the labelCol, defaults to None\n",
      "     |  \n",
      "     |  setParams(self, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  serializeToBundle(self, path, dataset=None)\n",
      "     |  \n",
      "     |  transform(self, dataset, params=None)\n",
      "     |      Transforms the input dataset with optional parameters.\n",
      "     |      \n",
      "     |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "     |      :param params: an optional param map that overrides embedded params.\n",
      "     |      :returns: transformed dataset\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3.0\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from pyspark.ml.base.Transformer:\n",
      "     |  \n",
      "     |  deserializeFromBundle(path)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "     class SpliceBaseEvaluator(builtins.object)\n",
      "     |  SpliceBaseEvaluator(spark, evaluator, supported_metrics, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, evaluator, supported_metrics, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class SpliceBinaryClassificationEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceBinaryClassificationEvaluator(spark, predictionCol='prediction', labelCol='label', confusion_matrix=True)\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceBinaryClassificationEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label', confusion_matrix=True)\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Evaluate actual vs Predicted in a dataframe\n",
      "     |      :param predictions_dataframe: the dataframe containing the label and the predicition\n",
      "     |  \n",
      "     |  plotROC(self, fittedEstimator, ax)\n",
      "     |      Plots the receiver operating characteristic curve for the trained classifier\n",
      "     |      :param fittedEstimator: fitted logistic regression model\n",
      "     |      :param ax: matplotlib axis object\n",
      "     |      :return: axis with ROC plot\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |\n",
      "    class SpliceMultiClassificationEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceMultiClassificationEvaluator(spark, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Base ModelEvaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceMultiClassificationEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SpliceRegressionEvaluator(SpliceBaseEvaluator)\n",
      "     |  SpliceRegressionEvaluator(spark, predictionCol='prediction', labelCol='label')\n",
      "     |  \n",
      "     |  Splice Regression Evaluator\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpliceRegressionEvaluator\n",
      "     |      SpliceBaseEvaluator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, spark, predictionCol='prediction', labelCol='label')\n",
      "     |      Constructor for SpliceBaseEvaluator\n",
      "     |      :param spark: spark from zeppelin\n",
      "     |      :param evaluator: evaluator class from spark\n",
      "     |      :param supported_metrics: supported metrics list\n",
      "     |      :param predictionCol: prediction column\n",
      "     |      :param labelCol: label column\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  get_results(self, as_dict=False)\n",
      "     |      Get Results\n",
      "     |      :param dict: whether to get results in a dict or not\n",
      "     |      :return: dictionary\n",
      "     |  \n",
      "     |  input(self, predictions_dataframe)\n",
      "     |      Input a dataframe\n",
      "     |      :param ev: evaluator class\n",
      "     |      :param predictions_dataframe: input df\n",
      "     |      :return: none\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from SpliceBaseEvaluator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    best_fit_distribution(data, col_name, bins, ax)\n",
      "        Model data by finding best fit distribution to data\n",
      "        :param data: DataFrame with one column containing the feature whose distribution is to be investigated\n",
      "        :param col_name: column name for feature\n",
      "        :param bins: number of bins to use in generating the histogram of this data\n",
      "        :param ax: axis to plot histogram on\n",
      "        :return: (best_distribution.name, best_params, best_sse)\n",
      "            best_distribution.name: string of the best distribution name\n",
      "            best_params: parameters for this distribution\n",
      "            best_sse: sum of squared errors for this distribution against the empirical pdf\n",
      "    \n",
      "    estimateCovariance(df, features_col='features')\n",
      "        Compute the covariance matrix for a given dataframe.\n",
      "            Note: The multi-dimensional covariance array should be calculated using outer products.  Don't forget to normalize the data by first subtracting the mean.\n",
      "        :param df: PySpark dataframe\n",
      "        :param features_col: name of the column with the features, defaults to 'features'\n",
      "        :return: np.ndarray: A multi-dimensional array where the number of rows and columns both equal the length of the arrays in the input dataframe.\n",
      "    \n",
      "    get_confusion_matrix(spark, TP, TN, FP, FN)\n",
      "        function that shows you a device called a confusion matrix... will be helpful when evaluating.\n",
      "        It allows you to see how well your model performs\n",
      "        :param TP: True Positives\n",
      "        :param TN: True Negatives\n",
      "        :param FP: False Positives\n",
      "        :param FN: False Negatives\n",
      "    \n",
      "    get_string_pipeline(df, cols_to_exclude, steps=['StringIndexer', 'OneHotEncoder', 'OneHotDummies'])\n",
      "        Generates a list of preprocessing stages\n",
      "        :param df: DataFrame including only the training data\n",
      "        :param cols_to_exclude: Column names we don't want to to include in the preprocessing (i.e. SUBJECT/ target column)\n",
      "        :param stages: preprocessing steps to take\n",
      "        :return:  (stages, Numeric_Columns)\n",
      "            stages: list of pipeline stages to be used in preprocessing\n",
      "            Numeric_Columns: list of columns that contain numeric features\n",
      "    \n",
      "    inspectTable(spliceMLCtx, sql, topN=5)\n",
      "        Inspect the values of the columns of the table (dataframe) returned from the sql query\n",
      "        :param spliceMLCtx: SpliceMLContext\n",
      "        :param sql: sql string to execute\n",
      "        :param topN: the number of most frequent elements of a column to return, defaults to 5\n",
      "    \n",
      "    make_pdf(dist, params, size=10000)\n",
      "        Generate distributions's Probability Distribution Function\n",
      "        :param dist: scipy.stats distribution object: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
      "        :param params: distribution parameters\n",
      "        :param size: how many data points to generate , defaults to 10000\n",
      "        :return: series of probability density function for this distribution\n",
      "    \n",
      "    pca_with_scores(df, k=10)\n",
      "        Computes the top `k` principal components, corresponding scores, and all eigenvalues.\n",
      "        Note:\n",
      "            All eigenvalues should be returned in sorted order (largest to smallest). `eigh` returns\n",
      "            each eigenvectors as a column.  This function should also return eigenvectors as columns.\n",
      "        :param df:  A Spark dataframe with a 'features' column, which (column) consists of DenseVectors.\n",
      "        :param k: The number of principal components to return., defaults to 10\n",
      "        :return:(eigenvectors, `RDD` of scores, eigenvalues).\n",
      "            Eigenvectors: multi-dimensional array where the number of\n",
      "            rows equals the length of the arrays in the input `RDD` and the number of columns equals`k`.\n",
      "            `RDD` of scores: has the same number of rows as `data` and consists of arrays of length `k`.\n",
      "            Eigenvalues is an array of length d (the number of features).\n",
      "    \n",
      "    postprocessing_pipeline(df, cols_to_exclude)\n",
      "        Assemble postprocessing pipeline to reconstruct original categorical indexed values from OneHotDummy Columns\n",
      "        :param df: DataFrame Including the original string Columns\n",
      "        :param cols_to_exclude: list of columns to exclude\n",
      "        :return: (reconstructers, String_Columns)\n",
      "            reconstructers: list of IndReconstructer stages\n",
      "            String_Columns: list of columns that are being reconstructed\n",
      "    \n",
      "    reconstructPCA(sql, df, pc, mean, std, originalColumns, fits, pcaColumn='pcaFeatures')\n",
      "        Reconstruct data from lower dimensional space after performing PCA\n",
      "        :param sql: SQLContext\n",
      "        :param df: PySpark DataFrame: inputted PySpark DataFrame\n",
      "        :param pc: numpy.ndarray: principal components projected onto\n",
      "        :param mean: numpy.ndarray: mean of original columns\n",
      "        :param std: numpy.ndarray: standard deviation of original columns\n",
      "        :param originalColumns: list: original column names\n",
      "        :param fits: fits of features returned from best_fit_distribution\n",
      "        :param pcaColumn: column in df that contains PCA features, defaults to 'pcaFeatures'\n",
      "        :return: dataframe containing reconstructed data\n",
      "    \n",
      "    varianceExplained(df, k=10)\n",
      "        returns the proportion of variance explained by `k` principal componenets. Calls the above PCA procedure\n",
      "        :param df: PySpark DataFrame\n",
      "        :param k: number of principal components , defaults to 10\n",
      "        :return: (proportion, principal_components, scores, eigenvalues)\n",
      "    \n",
      "    vector_assembler_pipeline(df, columns, doPCA=False, k=10)\n",
      "        After preprocessing String Columns, this function can be used to assemble a feature vector to be used for learning\n",
      "        creates the following stages: VectorAssembler -&gt; Standard Scalar [{ -&gt; PCA}]\n",
      "        :param df: DataFrame containing preprocessed Columns\n",
      "        :param columns: list of Column names of the preprocessed columns\n",
      "        :param doPCA:  Do you want to do PCA as part of the vector assembler? defaults to False\n",
      "        :param k:  Number of Principal Components to use, defaults to 10\n",
      "        :return: List of vector assembling stages\n",
      "</code>, <code>splicemachine.stats</code>, <code>get_spark_ui(port=None, spark_session=None)\n",
      "\n",
      "hide_toggle(toggle_next=False)\n",
      "    Function to add a toggle at the bottom of Jupyter Notebook cells to allow the entire cell to be collapsed.\n",
      "    :param toggle_next: Bool determine if the toggle should affect the current cell or the next cell\n",
      "    Usage: from splicemachine.stats.utilities import hide_toggle\n",
      "           hide_toggle()\n",
      "</code>, <code></code>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "print(soup.find_all('code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter defaults to port 8888, but can be changed\n",
    "### You can't access an IP unless it's port is made available\n",
    "\n",
    "<code>jupyter notebook </code> <-- 8888<br>\n",
    "<code>jupyter notebook --port=8889</code> <-- 8889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSH and SCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scp in /Users/benepstein/anaconda3/lib/python3.7/site-packages (0.13.3)\n",
      "Requirement already satisfied: paramiko in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from scp) (2.7.2)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko->scp) (1.4.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko->scp) (2.7)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko->scp) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from pynacl>=1.0.1->paramiko->scp) (1.12.3)\n",
      "Requirement already satisfied: six in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from pynacl>=1.0.1->paramiko->scp) (1.15.0)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cryptography>=2.5->paramiko->scp) (0.24.0)\n",
      "Requirement already satisfied: pycparser in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko->scp) (2.19)\n",
      "Requirement already satisfied: paramiko in /Users/benepstein/anaconda3/lib/python3.7/site-packages (2.7.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko) (2.7)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko) (1.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from paramiko) (3.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cryptography>=2.5->paramiko) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cryptography>=2.5->paramiko) (0.24.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cryptography>=2.5->paramiko) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /Users/benepstein/anaconda3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.5->paramiko) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q scp\n",
    "!pip install -q paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "\n",
    "my test file2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python equivalent of \n",
    "<code>scp -i $ex_pem test.txt $ex_host:/home/ubuntu/test.txt</code>\n",
    "\n",
    "### Where\n",
    "* ex_pem is your PEM file\n",
    "* ex_host is your host IP address\n",
    "\n",
    "[src](https://gist.github.com/batok/2352501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paramiko import SSHClient, RSAKey, AutoAddPolicy\n",
    "from scp import SCPClient\n",
    "\n",
    "# SSH Client\n",
    "ssh = SSHClient()\n",
    "\n",
    "# EC2 Host and Private Key\n",
    "k = RSAKey.from_private_key_file(open('pem_loc.txt').read().strip())\n",
    "host = open('ec2_host.txt').read().strip()\n",
    "\n",
    "# Add new host if unknown\n",
    "ssh.set_missing_host_key_policy(AutoAddPolicy())\n",
    "\n",
    "ssh.connect(host,username='ubuntu',pkey=k)\n",
    "\n",
    "# SCPCLient takes a paramiko transport as an argument\n",
    "scp = SCPClient(ssh.get_transport())\n",
    "\n",
    "# Uploading the 'test' directory with its content in the\n",
    "# home (~) remote directory\n",
    "scp.put('test.txt', recursive=True, remote_path='~')\n",
    "\n",
    "scp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boto3 and AWS S3\n",
    "\n",
    "[src](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file('splice-demo', 'customers-4000.csv', 'customers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read directly with Pandas (assuming it's public)\n",
    "\n",
    "### For private files you'll need to provide credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>Full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eric</td>\n",
       "      <td>Nash</td>\n",
       "      <td>Eric Nash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Owen</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>Owen Clarkson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Simon</td>\n",
       "      <td>Stewart</td>\n",
       "      <td>Simon Stewart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tim</td>\n",
       "      <td>Paterson</td>\n",
       "      <td>Tim Paterson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Paige</td>\n",
       "      <td>Julia Paige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3996</td>\n",
       "      <td>Luke</td>\n",
       "      <td>Mackenzie</td>\n",
       "      <td>Luke Mackenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3997</td>\n",
       "      <td>William</td>\n",
       "      <td>Duncan</td>\n",
       "      <td>William Duncan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3998</td>\n",
       "      <td>Boris</td>\n",
       "      <td>Watson</td>\n",
       "      <td>Boris Watson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3999</td>\n",
       "      <td>Nicola</td>\n",
       "      <td>Randall</td>\n",
       "      <td>Nicola Randall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>4000</td>\n",
       "      <td>Boris</td>\n",
       "      <td>Bailey</td>\n",
       "      <td>Boris Bailey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    First       Last            Full\n",
       "0        1     Eric       Nash       Eric Nash\n",
       "1        2     Owen   Clarkson   Owen Clarkson\n",
       "2        3    Simon    Stewart   Simon Stewart\n",
       "3        4      Tim   Paterson    Tim Paterson\n",
       "4        5    Julia      Paige     Julia Paige\n",
       "...    ...      ...        ...             ...\n",
       "3995  3996     Luke  Mackenzie  Luke Mackenzie\n",
       "3996  3997  William     Duncan  William Duncan\n",
       "3997  3998    Boris     Watson    Boris Watson\n",
       "3998  3999   Nicola    Randall  Nicola Randall\n",
       "3999  4000    Boris     Bailey    Boris Bailey\n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('s3://splice-demo/customers-4000.csv', header=None, names=['ID', 'First', 'Last', \"Full\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['ID'] <= 100]\n",
    "df2.to_csv('small_customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file('small_customers.csv', 'splice-demo', 'small_customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
